---
title: "Work"
author: "Frida Gomam"
date: 2020-12-01T21:13:14-05:00
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
menu: main
---



<div id="workshop-3---questions" class="section level1">
<h1>Workshop 3 - Questions</h1>
<pre class="r"><code>#load the data from lending club
lendingclub &lt;- read_csv((here::here(&quot;lending_club_raw_data.csv&quot;)))
lendingclub_clean &lt;- lendingclub[,c(1:19)]
head(lendingclub_clean,10)</code></pre>
<pre><code>## # A tibble: 10 x 19
##    int_rate loan_amnt `term (months)` installment   dti delinq_2yrs annual_inc
##       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1     0.11      5000              36       163.  27.6            0      24000
##  2     0.15      2500              60        59.8  1              0      30000
##  3     0.16      2400              36        84.3  8.72           0      12252
##  4     0.13     10000              36       339.  20              0      49200
##  5     0.13      3000              60        67.8 17.9            0      80000
##  6     0.08      5000              36       156.  11.2            0      36000
##  7     0.16      7000              60       170.  23.5            0      47004
##  8     0.19      3000              36       109.   5.35           0      48000
##  9     0.21      5600              60       152.   5.55           0      40000
## 10     0.13      5375              60       121.  18.1            0      15000
## # ... with 12 more variables: grade &lt;chr&gt;, emp_title &lt;chr&gt;, emp_length &lt;chr&gt;,
## #   home_ownership &lt;chr&gt;, verification_status &lt;chr&gt;, issue_d &lt;chr&gt;,
## #   zip_code &lt;chr&gt;, addr_state &lt;chr&gt;, loan_status &lt;chr&gt;, desc &lt;chr&gt;,
## #   purpose &lt;chr&gt;, title &lt;chr&gt;</code></pre>
<div id="lets-go-to-the-full-dataset.-create-a" class="section level2">
<h2>1. Let’s go to the full dataset. Create a […]</h2>
<pre class="r"><code>#create the column in which we will store the binary values of the loan status
loan_stat_binary &lt;- lendingclub_clean$loan_status
lendingclub_clean &lt;- cbind(lendingclub_clean,loan_stat_binary)

#create the binary values
lendingclub_clean &lt;- lendingclub_clean %&gt;%
      mutate(loan_stat_binary = ifelse(loan_stat_binary == &quot;Charged Off&quot;,1,0))
colnames(lendingclub_clean)</code></pre>
<pre><code>##  [1] &quot;int_rate&quot;            &quot;loan_amnt&quot;           &quot;term (months)&quot;      
##  [4] &quot;installment&quot;         &quot;dti&quot;                 &quot;delinq_2yrs&quot;        
##  [7] &quot;annual_inc&quot;          &quot;grade&quot;               &quot;emp_title&quot;          
## [10] &quot;emp_length&quot;          &quot;home_ownership&quot;      &quot;verification_status&quot;
## [13] &quot;issue_d&quot;             &quot;zip_code&quot;            &quot;addr_state&quot;         
## [16] &quot;loan_status&quot;         &quot;desc&quot;                &quot;purpose&quot;            
## [19] &quot;title&quot;               &quot;loan_stat_binary&quot;</code></pre>
</div>
<div id="investigate-how-often-do-loans-default-create-a-correlation" class="section level2">
<h2>2. Investigate, how often do loans default? Create a correlation […]</h2>
<pre class="r"><code>#select the quantitative variables and create a correlation plot
lendingclub_quant &lt;- lendingclub_clean[c(1:2,4:5,7,20)]
ggpairs(lendingclub_quant)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/correlation%20matrix-1.png" width="672" />
## 3. Create a smaller dataset (10,000 observations) to investigate […]</p>
<pre class="r"><code>#create a sample from the data of 10,000 observations
lendingclub_sample &lt;- sample(lendingclub_clean,10000)

#create a logistic regression with the two required variables
logistic_reg &lt;- glm(loan_stat_binary ~
                    loan_amnt + dti,
                    family = binomial,
                    data = lendingclub_sample)
summary(logistic_reg)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + dti, family = binomial, 
##     data = lendingclub_sample)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7487  -0.5753  -0.5383  -0.4960   2.1647  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.265e+00  8.349e-02 -27.130  &lt; 2e-16 ***
## loan_amnt    1.556e-05  3.914e-06   3.975 7.04e-05 ***
## dti          2.106e-02  4.583e-03   4.596 4.32e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7327.4  on 8958  degrees of freedom
## Residual deviance: 7289.0  on 8956  degrees of freedom
##   (1041 observations deleted due to missingness)
## AIC: 7295
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>#use our model to predict the data using our current data set and store it in a df
pred_log_reg &lt;- predict(logistic_reg, newdata = lendingclub_sample, type = &quot;response&quot;)
pred_log_reg_df &lt;- data.frame(pred_log_reg)

#isolate the binary variables to create the upcoming ROC curves
sample_binary_loan_stat &lt;- lendingclub_sample[&quot;loan_stat_binary&quot;]

#create a ROC curve to assess the model&#39;s effectiveness
ROC_logreg &lt;- cbind(sample_binary_loan_stat,pred_log_reg_df)
ROC_logreg_clean &lt;- na.omit(ROC_logreg)
PRROC_obj &lt;- roc.curve(scores.class0 = ROC_logreg_clean$pred_log_reg, weights.class0=ROC_logreg_clean$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/first%20logistic%20reg-1.png" width="672" /></p>
<pre class="r"><code>#create a confusion matrix with a 0.5 cut-off
pred_log_reg_cut_off_50 &lt;- ifelse(pred_log_reg_df &gt; 0.5, 1, 0)
pred_log_reg_cut_off_50_df &lt;- data.frame(pred_log_reg_cut_off_50)

table(lendingclub_sample$loan_stat_binary, pred_log_reg_cut_off_50)</code></pre>
<pre><code>##    pred_log_reg_cut_off_50
##        0
##   0 7685
##   1 1274</code></pre>
<pre class="r"><code>#create a confusion matrix with a 0.25 cut-off
pred_log_reg_cut_off_25 &lt;- ifelse(pred_log_reg_df &gt; 0.25, 1, 0)
pred_log_reg_cut_off_25_df &lt;- data.frame(pred_log_reg_cut_off_25)

table(lendingclub_sample$loan_stat_binary, pred_log_reg_cut_off_25)</code></pre>
<pre><code>##    pred_log_reg_cut_off_25
##        0
##   0 7685
##   1 1274</code></pre>
<pre class="r"><code>#create a confusion matrix with a 0.15 cut-off
pred_log_reg_cut_off_15 &lt;- ifelse(pred_log_reg_df &gt; 0.15, 1, 0)
pred_log_reg_cut_off_15_df &lt;- data.frame(pred_log_reg_cut_off_15)

table(lendingclub_sample$loan_stat_binary, pred_log_reg_cut_off_15)</code></pre>
<pre><code>##    pred_log_reg_cut_off_15
##        0    1
##   0 5163 2522
##   1  742  532</code></pre>
</div>
<div id="lets-try-to-improve-the-models-accuracy-by-adding-more-variables-including" class="section level2">
<h2>5. Let’s try to improve the model’s accuracy by adding more variables, including […]</h2>
<pre class="r"><code>#reduce the number of categories for several variables
lendingclub_sample$emp_length &lt;- recode(lendingclub_sample$emp_length, &#39;&lt; 1 year&#39; = &#39;&lt; 2 years&#39;, &#39;1 year&#39; = &#39;&lt; 2 years&#39;, &#39;2 years&#39; = &#39;2-5 years&#39;, &#39;3 years&#39; = &#39;2-5 years&#39;, &#39;4 years&#39; = &#39;2-5 years&#39;, &#39;5 years&#39; = &#39;2-5 years&#39;, &#39;6 years&#39; = &#39;&gt; 5 years&#39;,&#39;7 years&#39; = &#39;&gt; 5 years&#39;,&#39;8 years&#39; = &#39;&gt; 5 years&#39;,&#39;9 years&#39; = &#39;&gt; 5 years&#39;,&#39;10+ years&#39; = &#39;&gt; 5 years&#39;)

lendingclub_sample$delinq_2yrs &lt;- recode(lendingclub_sample$delinq_2yrs, &#39;0&#39; = &#39;&lt; 2&#39;, &#39;1&#39; = &#39;&lt; 2&#39;, &#39;2&#39; = &#39;2-5&#39;, &#39;3&#39; = &#39;2-5&#39;, &#39;4&#39; = &#39;2-5&#39;, &#39;5&#39; = &#39;2-5&#39;, &#39;6&#39; = &#39;&gt; 5&#39;,&#39;7&#39; = &#39;&gt; 5&#39;,&#39;8&#39; = &#39;&gt; 5&#39;,&#39;9&#39; = &#39;&gt; 5&#39;)

lendingclub_sample$verification_status &lt;- recode(lendingclub_sample$verification_status, &#39;Source Verified&#39; = &#39;Verified&#39;, &#39;Verified&#39; = &#39;Verified&#39;, &#39;Not Verified&#39; = &#39;Not Verified&#39;)

#select the relevant columns of our df
lendingclub_sample &lt;- lendingclub_sample[,c(1:20)]

#create dummies for the categorical variables
lendingclub_sample_dummied &lt;- dummy_cols(lendingclub_sample, select_columns = c(&#39;term (months)&#39;,&#39;grade&#39;,&#39;emp_length&#39;,&#39;home_ownership&#39;,&#39;verification_status&#39;,&#39;purpose&#39;))

#delete the useless dummies
drops &lt;- c(&quot;term (months)_60&quot;,&quot;term (months)_NA&quot;,&quot;grade_G&quot;,&quot;grade_NA&quot;,&quot;emp_length_&gt; 5 years&quot;,&quot;emp_length_n/a&quot;,&quot;emp_length_NA&quot;,&quot;home_ownership_OTHER&quot;,&quot;home_ownership_NA&quot;,&quot;verification_status_Verified&quot;,&quot;verification_status_NA&quot;,&quot;purpose_other&quot;,&quot;purpose_NA&quot;)
lendingclub_sample_dummied_cleaned &lt;- lendingclub_sample_dummied[ , !(names(lendingclub_sample_dummied) %in% drops)]
lendingclub_sample_dummied_cleaned_names &lt;- clean_names(lendingclub_sample_dummied_cleaned)

#display the names of the selected variables
colnames(lendingclub_sample_dummied_cleaned_names)</code></pre>
<pre><code>##  [1] &quot;int_rate&quot;                         &quot;loan_amnt&quot;                       
##  [3] &quot;term_months&quot;                      &quot;installment&quot;                     
##  [5] &quot;dti&quot;                              &quot;delinq_2yrs&quot;                     
##  [7] &quot;annual_inc&quot;                       &quot;grade&quot;                           
##  [9] &quot;emp_title&quot;                        &quot;emp_length&quot;                      
## [11] &quot;home_ownership&quot;                   &quot;verification_status&quot;             
## [13] &quot;issue_d&quot;                          &quot;zip_code&quot;                        
## [15] &quot;addr_state&quot;                       &quot;loan_status&quot;                     
## [17] &quot;desc&quot;                             &quot;purpose&quot;                         
## [19] &quot;title&quot;                            &quot;loan_stat_binary&quot;                
## [21] &quot;term_months_36&quot;                   &quot;grade_a&quot;                         
## [23] &quot;grade_b&quot;                          &quot;grade_c&quot;                         
## [25] &quot;grade_d&quot;                          &quot;grade_e&quot;                         
## [27] &quot;grade_f&quot;                          &quot;emp_length_2_years&quot;              
## [29] &quot;emp_length_2_5_years&quot;             &quot;home_ownership_mortgage&quot;         
## [31] &quot;home_ownership_none&quot;              &quot;home_ownership_own&quot;              
## [33] &quot;home_ownership_rent&quot;              &quot;verification_status_not_verified&quot;
## [35] &quot;purpose_car&quot;                      &quot;purpose_credit_card&quot;             
## [37] &quot;purpose_debt_consolidation&quot;       &quot;purpose_educational&quot;             
## [39] &quot;purpose_home_improvement&quot;         &quot;purpose_house&quot;                   
## [41] &quot;purpose_major_purchase&quot;           &quot;purpose_medical&quot;                 
## [43] &quot;purpose_moving&quot;                   &quot;purpose_renewable_energy&quot;        
## [45] &quot;purpose_small_business&quot;           &quot;purpose_vacation&quot;                
## [47] &quot;purpose_wedding&quot;</code></pre>
<pre class="r"><code>#create a new logistic regression with the new variables
logistic_reg_2 &lt;- glm(loan_stat_binary ~ 
                        loan_amnt + 
                        installment + 
                        dti +
                        annual_inc +
                        term_months_36 + 
                        grade_a + grade_b + grade_c + grade_d + grade_e + grade_f + 
                        emp_length_2_years + emp_length_2_5_years + 
                        home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
                        verification_status_not_verified + 
                        purpose_car + purpose_credit_card + purpose_debt_consolidation + purpose_educational + purpose_home_improvement + purpose_house + purpose_major_purchase + purpose_medical + purpose_moving + purpose_renewable_energy + purpose_small_business + purpose_vacation + purpose_wedding,
                        family = binomial,
                        data = lendingclub_sample_dummied_cleaned_names)
summary(logistic_reg_2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + installment + dti + 
##     annual_inc + term_months_36 + grade_a + grade_b + grade_c + 
##     grade_d + grade_e + grade_f + emp_length_2_years + emp_length_2_5_years + 
##     home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
##     verification_status_not_verified + purpose_car + purpose_credit_card + 
##     purpose_debt_consolidation + purpose_educational + purpose_home_improvement + 
##     purpose_house + purpose_major_purchase + purpose_medical + 
##     purpose_moving + purpose_renewable_energy + purpose_small_business + 
##     purpose_vacation + purpose_wedding, family = binomial, data = lendingclub_sample_dummied_cleaned_names)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3612  -0.6017  -0.4646  -0.3168   3.5244  
## 
## Coefficients:
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                       2.270e-01  7.141e-01   0.318 0.750554    
## loan_amnt                        -2.760e-05  2.599e-05  -1.062 0.288414    
## installment                       8.750e-04  8.918e-04   0.981 0.326544    
## dti                               1.399e-02  4.924e-03   2.842 0.004483 ** 
## annual_inc                       -4.138e-06  9.671e-07  -4.279 1.88e-05 ***
## term_months_36                   -7.411e-01  1.386e-01  -5.348 8.90e-08 ***
## grade_a                          -1.526e+00  2.820e-01  -5.413 6.20e-08 ***
## grade_b                          -8.460e-01  2.658e-01  -3.183 0.001459 ** 
## grade_c                          -4.787e-01  2.636e-01  -1.816 0.069337 .  
## grade_d                          -3.539e-01  2.616e-01  -1.353 0.176090    
## grade_e                          -5.002e-01  2.670e-01  -1.873 0.061015 .  
## grade_f                          -4.317e-02  2.853e-01  -0.151 0.879703    
## emp_length_2_years               -2.133e-02  8.618e-02  -0.247 0.804557    
## emp_length_2_5_years             -2.001e-01  7.221e-02  -2.770 0.005597 ** 
## home_ownership_mortgage          -5.374e-01  6.498e-01  -0.827 0.408261    
## home_ownership_own               -1.636e-01  6.567e-01  -0.249 0.803238    
## home_ownership_rent              -3.141e-01  6.491e-01  -0.484 0.628497    
## verification_status_not_verified -4.739e-02  6.978e-02  -0.679 0.497053    
## purpose_car                      -4.326e-01  1.975e-01  -2.190 0.028532 *  
## purpose_credit_card              -5.335e-01  1.419e-01  -3.760 0.000170 ***
## purpose_debt_consolidation       -2.313e-01  1.071e-01  -2.160 0.030743 *  
## purpose_educational               2.879e-01  3.022e-01   0.953 0.340789    
## purpose_home_improvement         -2.138e-01  1.566e-01  -1.366 0.171983    
## purpose_house                    -2.812e-01  3.347e-01  -0.840 0.400822    
## purpose_major_purchase           -1.768e-01  1.723e-01  -1.026 0.304795    
## purpose_medical                  -1.518e-01  2.667e-01  -0.569 0.569402    
## purpose_moving                    5.225e-02  2.598e-01   0.201 0.840604    
## purpose_renewable_energy          1.029e+00  4.622e-01   2.226 0.026042 *  
## purpose_small_business            4.149e-01  1.518e-01   2.733 0.006275 ** 
## purpose_vacation                 -2.223e-01  3.725e-01  -0.597 0.550539    
## purpose_wedding                  -9.922e-01  2.725e-01  -3.641 0.000271 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7327.4  on 8958  degrees of freedom
## Residual deviance: 6805.7  on 8928  degrees of freedom
##   (1041 observations deleted due to missingness)
## AIC: 6867.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>#predict with the log reg 2 the default probability with our current dataset and store in a df
pred_log_reg_2 &lt;- predict(logistic_reg_2, newdata = lendingclub_sample_dummied_cleaned_names, type = &quot;response&quot;)
pred_log_reg_2_df &lt;- data.frame(pred_log_reg_2)

#isolate the binary variables to create the ROC curve
sample_binary_cleaned_loan_stat &lt;- lendingclub_sample_dummied_cleaned_names[&quot;loan_stat_binary&quot;]

#create a ROC curve to assess the model&#39;s effectiveness
ROC_logreg_2 &lt;- cbind(sample_binary_cleaned_loan_stat,pred_log_reg_2_df)
ROC_logreg_2_clean &lt;- na.omit(ROC_logreg_2)
PRROC_obj &lt;- roc.curve(scores.class0 = ROC_logreg_2_clean$pred_log_reg, weights.class0=ROC_logreg_2_clean$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/second%20logistic%20reg-1.png" width="672" /></p>
<pre class="r"><code>#create interaction terms between the home ownership status and the credit rating
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_mort_E &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_mortgage * lendingclub_sample_dummied_cleaned_names$grade_e
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_own_E &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_own * lendingclub_sample_dummied_cleaned_names$grade_e
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_rent_E &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_rent * lendingclub_sample_dummied_cleaned_names$grade_e
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_mort_F &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_mortgage * lendingclub_sample_dummied_cleaned_names$grade_f
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_own_F &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_own * lendingclub_sample_dummied_cleaned_names$grade_f
lendingclub_sample_dummied_cleaned_names$home_ownsh_x_rent_F &lt;- lendingclub_sample_dummied_cleaned_names$home_ownership_rent * lendingclub_sample_dummied_cleaned_names$grade_f

#create interaction terms between the loan term and the loan amount
lendingclub_sample_dummied_cleaned_names$loan_amnt_x_term_36 &lt;- lendingclub_sample_dummied_cleaned_names$loan_amnt * lendingclub_sample_dummied_cleaned_names$term_months_36

#create a new logistic regression with our interaction terms
logistic_reg_3 &lt;- glm(loan_stat_binary ~ 
                        loan_amnt + 
                        installment + 
                        dti +
                        annual_inc +
                        term_months_36 + 
                        grade_a + grade_b + grade_c + grade_d + grade_e + grade_f + 
                        emp_length_2_years + emp_length_2_5_years + 
                        home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
                        verification_status_not_verified + 
                        purpose_car + purpose_credit_card + purpose_debt_consolidation + purpose_educational + purpose_home_improvement + purpose_house + purpose_major_purchase + purpose_medical + purpose_moving + purpose_renewable_energy + purpose_small_business + purpose_vacation + purpose_wedding +
                        home_ownsh_x_mort_E + home_ownsh_x_own_E + home_ownsh_x_rent_E +
                        home_ownsh_x_mort_F + home_ownsh_x_own_F + home_ownsh_x_rent_F +
                        loan_amnt_x_term_36,
                        family = binomial,
                        data = lendingclub_sample_dummied_cleaned_names)

summary(logistic_reg_3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + installment + dti + 
##     annual_inc + term_months_36 + grade_a + grade_b + grade_c + 
##     grade_d + grade_e + grade_f + emp_length_2_years + emp_length_2_5_years + 
##     home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
##     verification_status_not_verified + purpose_car + purpose_credit_card + 
##     purpose_debt_consolidation + purpose_educational + purpose_home_improvement + 
##     purpose_house + purpose_major_purchase + purpose_medical + 
##     purpose_moving + purpose_renewable_energy + purpose_small_business + 
##     purpose_vacation + purpose_wedding + home_ownsh_x_mort_E + 
##     home_ownsh_x_own_E + home_ownsh_x_rent_E + home_ownsh_x_mort_F + 
##     home_ownsh_x_own_F + home_ownsh_x_rent_F + loan_amnt_x_term_36, 
##     family = binomial, data = lendingclub_sample_dummied_cleaned_names)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3627  -0.5993  -0.4646  -0.3163   3.5605  
## 
## Coefficients: (1 not defined because of singularities)
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                      -5.665e-01  8.338e-01  -0.679 0.496878    
## loan_amnt                        -1.709e-04  5.592e-05  -3.055 0.002250 ** 
## installment                       6.917e-03  2.263e-03   3.057 0.002235 ** 
## dti                               1.309e-02  4.940e-03   2.650 0.008060 ** 
## annual_inc                       -4.209e-06  9.700e-07  -4.340 1.43e-05 ***
## term_months_36                   -7.326e-01  1.392e-01  -5.264 1.41e-07 ***
## grade_a                          -8.918e-01  3.575e-01  -2.495 0.012607 *  
## grade_b                          -2.632e-01  3.342e-01  -0.788 0.430969    
## grade_c                           2.602e-02  3.173e-01   0.082 0.934649    
## grade_d                           6.962e-02  3.005e-01   0.232 0.816774    
## grade_e                          -1.397e-01  3.124e-01  -0.447 0.654719    
## grade_f                           1.242e+01  1.970e+02   0.063 0.949733    
## emp_length_2_years               -1.585e-02  8.629e-02  -0.184 0.854258    
## emp_length_2_5_years             -1.944e-01  7.232e-02  -2.688 0.007178 ** 
## home_ownership_mortgage          -1.986e-01  7.615e-01  -0.261 0.794214    
## home_ownership_own                1.599e-01  7.680e-01   0.208 0.835039    
## home_ownership_rent               1.842e-03  7.607e-01   0.002 0.998068    
## verification_status_not_verified -4.727e-02  6.990e-02  -0.676 0.498950    
## purpose_car                      -4.338e-01  1.973e-01  -2.199 0.027861 *  
## purpose_credit_card              -5.226e-01  1.420e-01  -3.681 0.000232 ***
## purpose_debt_consolidation       -2.139e-01  1.072e-01  -1.995 0.046069 *  
## purpose_educational               2.942e-01  3.018e-01   0.975 0.329627    
## purpose_home_improvement         -1.919e-01  1.566e-01  -1.226 0.220205    
## purpose_house                    -2.708e-01  3.358e-01  -0.807 0.419920    
## purpose_major_purchase           -1.761e-01  1.721e-01  -1.023 0.306151    
## purpose_medical                  -1.473e-01  2.663e-01  -0.553 0.580348    
## purpose_moving                    4.619e-02  2.592e-01   0.178 0.858597    
## purpose_renewable_energy          1.042e+00  4.637e-01   2.246 0.024686 *  
## purpose_small_business            4.265e-01  1.524e-01   2.798 0.005142 ** 
## purpose_vacation                 -2.413e-01  3.719e-01  -0.649 0.516477    
## purpose_wedding                  -9.746e-01  2.723e-01  -3.578 0.000346 ***
## home_ownsh_x_mort_E              -1.029e-01  2.236e-01  -0.460 0.645422    
## home_ownsh_x_own_E               -6.407e-02  4.028e-01  -0.159 0.873640    
## home_ownsh_x_rent_E                      NA         NA      NA       NA    
## home_ownsh_x_mort_F              -1.241e+01  1.970e+02  -0.063 0.949744    
## home_ownsh_x_own_F               -1.203e+01  1.970e+02  -0.061 0.951296    
## home_ownsh_x_rent_F              -1.225e+01  1.970e+02  -0.062 0.950416    
## loan_amnt_x_term_36              -6.565e-05  2.233e-05  -2.940 0.003279 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7327.4  on 8958  degrees of freedom
## Residual deviance: 6794.9  on 8922  degrees of freedom
##   (1041 observations deleted due to missingness)
## AIC: 6868.9
## 
## Number of Fisher Scoring iterations: 10</code></pre>
<pre class="r"><code>#predict with the log reg 2 the default probability with our current dataset and store in a df
pred_log_reg_3 &lt;- predict(logistic_reg_3, newdata = lendingclub_sample_dummied_cleaned_names, type = &quot;response&quot;)
pred_log_reg_3_df &lt;- data.frame(pred_log_reg_3)

#plot a ROC curve for log reg 3
ROC_logreg_3 &lt;- cbind(sample_binary_cleaned_loan_stat,pred_log_reg_3_df)
ROC_logreg_3_clean &lt;- na.omit(ROC_logreg_3)
PRROC_obj &lt;- roc.curve(scores.class0 = ROC_logreg_3_clean$pred_log_reg, weights.class0=ROC_logreg_3_clean$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/interaction%20terms%20and%20third%20log%20reg-1.png" width="672" /></p>
<pre class="r"><code>#create a logistic regression removing the variables yielding only NAs
logistic_reg_4 &lt;- glm(loan_stat_binary ~ 
                        loan_amnt + 
                        installment + 
                        dti +
                        annual_inc +
                        term_months_36 + 
                        grade_a + grade_b + grade_c + grade_d + grade_e + grade_f + 
                        emp_length_2_years + emp_length_2_5_years + 
                        home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
                        verification_status_not_verified + 
                        purpose_car + purpose_credit_card + purpose_debt_consolidation + purpose_educational + purpose_home_improvement + purpose_house + purpose_major_purchase + purpose_medical + purpose_moving + purpose_renewable_energy + purpose_small_business + purpose_vacation + purpose_wedding +
                        home_ownsh_x_mort_E + home_ownsh_x_own_E + home_ownsh_x_rent_E +
                        home_ownsh_x_mort_F + home_ownsh_x_own_F +
                        loan_amnt_x_term_36,
                        family = binomial,
                        data = lendingclub_sample_dummied_cleaned_names)

summary(logistic_reg_4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + installment + dti + 
##     annual_inc + term_months_36 + grade_a + grade_b + grade_c + 
##     grade_d + grade_e + grade_f + emp_length_2_years + emp_length_2_5_years + 
##     home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
##     verification_status_not_verified + purpose_car + purpose_credit_card + 
##     purpose_debt_consolidation + purpose_educational + purpose_home_improvement + 
##     purpose_house + purpose_major_purchase + purpose_medical + 
##     purpose_moving + purpose_renewable_energy + purpose_small_business + 
##     purpose_vacation + purpose_wedding + home_ownsh_x_mort_E + 
##     home_ownsh_x_own_E + home_ownsh_x_rent_E + home_ownsh_x_mort_F + 
##     home_ownsh_x_own_F + loan_amnt_x_term_36, family = binomial, 
##     data = lendingclub_sample_dummied_cleaned_names)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3650  -0.5995  -0.4647  -0.3163   3.5684  
## 
## Coefficients: (1 not defined because of singularities)
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                      -2.665e-01  7.337e-01  -0.363 0.716470    
## loan_amnt                        -1.715e-04  5.591e-05  -3.068 0.002154 ** 
## installment                       6.945e-03  2.262e-03   3.070 0.002139 ** 
## dti                               1.304e-02  4.940e-03   2.640 0.008290 ** 
## annual_inc                       -4.232e-06  9.691e-07  -4.367 1.26e-05 ***
## term_months_36                   -7.357e-01  1.392e-01  -5.287 1.25e-07 ***
## grade_a                          -8.899e-01  3.574e-01  -2.490 0.012786 *  
## grade_b                          -2.608e-01  3.341e-01  -0.781 0.435010    
## grade_c                           2.916e-02  3.172e-01   0.092 0.926756    
## grade_d                           7.098e-02  3.005e-01   0.236 0.813253    
## grade_e                          -1.372e-01  3.124e-01  -0.439 0.660640    
## grade_f                           1.988e-01  3.358e-01   0.592 0.553845    
## emp_length_2_years               -1.418e-02  8.624e-02  -0.164 0.869430    
## emp_length_2_5_years             -1.943e-01  7.232e-02  -2.686 0.007234 ** 
## home_ownership_mortgage          -4.987e-01  6.503e-01  -0.767 0.443165    
## home_ownership_own               -1.403e-01  6.579e-01  -0.213 0.831085    
## home_ownership_rent              -2.998e-01  6.490e-01  -0.462 0.644141    
## verification_status_not_verified -4.796e-02  6.990e-02  -0.686 0.492635    
## purpose_car                      -4.333e-01  1.973e-01  -2.196 0.028081 *  
## purpose_credit_card              -5.232e-01  1.420e-01  -3.686 0.000228 ***
## purpose_debt_consolidation       -2.152e-01  1.072e-01  -2.007 0.044743 *  
## purpose_educational               2.899e-01  3.017e-01   0.961 0.336629    
## purpose_home_improvement         -1.923e-01  1.566e-01  -1.228 0.219439    
## purpose_house                    -2.708e-01  3.358e-01  -0.807 0.419925    
## purpose_major_purchase           -1.768e-01  1.721e-01  -1.027 0.304456    
## purpose_medical                  -1.472e-01  2.664e-01  -0.552 0.580608    
## purpose_moving                    4.682e-02  2.593e-01   0.181 0.856692    
## purpose_renewable_energy          1.039e+00  4.633e-01   2.242 0.024933 *  
## purpose_small_business            4.335e-01  1.522e-01   2.849 0.004386 ** 
## purpose_vacation                 -2.394e-01  3.719e-01  -0.644 0.519746    
## purpose_wedding                  -9.749e-01  2.724e-01  -3.579 0.000344 ***
## home_ownsh_x_mort_E              -1.039e-01  2.236e-01  -0.465 0.642164    
## home_ownsh_x_own_E               -6.454e-02  4.028e-01  -0.160 0.872703    
## home_ownsh_x_rent_E                      NA         NA      NA       NA    
## home_ownsh_x_mort_F              -1.946e-01  3.074e-01  -0.633 0.526696    
## home_ownsh_x_own_F                1.877e-01  7.269e-01   0.258 0.796239    
## loan_amnt_x_term_36              -6.547e-05  2.232e-05  -2.933 0.003353 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7327.4  on 8958  degrees of freedom
## Residual deviance: 6796.9  on 8923  degrees of freedom
##   (1041 observations deleted due to missingness)
## AIC: 6868.9
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>#predicting default probabilities on our current dataset and store in a df
pred_log_reg_4 &lt;- predict(logistic_reg_4, newdata = lendingclub_sample_dummied_cleaned_names, type = &quot;response&quot;)
pred_log_reg_4_df &lt;- data.frame(pred_log_reg_4)

#create a ROC curve
ROC_logreg_4 &lt;- cbind(sample_binary_cleaned_loan_stat,pred_log_reg_4_df)
ROC_logreg_4_clean &lt;- na.omit(ROC_logreg_4)
PRROC_obj &lt;- roc.curve(scores.class0 = ROC_logreg_4_clean$pred_log_reg, weights.class0=ROC_logreg_4_clean$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/fourth%20logistic%20reg-1.png" width="672" /></p>
</div>
<div id="the-model-we-have-estimated-in-the-previous-step-used-the-same-data-to-estimate" class="section level2">
<h2>6. The model we have estimated in the previous step used the same data to estimate […]</h2>
<pre class="r"><code>#select 60% of the sample
smp_size &lt;- floor(0.6 * nrow(lendingclub_sample_dummied_cleaned_names))

#set a seed to make the sample reproductible
set.seed(123)
train_ind &lt;- sample(seq_len(nrow(lendingclub_sample_dummied_cleaned_names)), size = smp_size)

#assign the training and the testing datasets to new dfs
train &lt;- lendingclub_sample_dummied_cleaned_names[train_ind, ]
test &lt;- lendingclub_sample_dummied_cleaned_names[-train_ind, ]

#take the model we created before and train in on the training data
logistic_reg_trained &lt;- glm(loan_stat_binary ~ 
                        loan_amnt + 
                        installment + 
                        dti +
                        annual_inc +
                        term_months_36 + 
                        grade_a + grade_b + grade_c + grade_d + grade_e + grade_f + 
                        emp_length_2_years + emp_length_2_5_years + 
                        home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
                        verification_status_not_verified + 
                        purpose_car + purpose_credit_card + purpose_debt_consolidation + purpose_educational + purpose_home_improvement + purpose_house + purpose_major_purchase + purpose_medical + purpose_moving + purpose_renewable_energy + purpose_small_business + purpose_vacation + purpose_wedding +
                        home_ownsh_x_mort_E + home_ownsh_x_own_E + home_ownsh_x_rent_E +
                        home_ownsh_x_mort_F + home_ownsh_x_own_F +
                        loan_amnt_x_term_36,
                        family = binomial,
                        data = train)
summary(logistic_reg_trained)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + installment + dti + 
##     annual_inc + term_months_36 + grade_a + grade_b + grade_c + 
##     grade_d + grade_e + grade_f + emp_length_2_years + emp_length_2_5_years + 
##     home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
##     verification_status_not_verified + purpose_car + purpose_credit_card + 
##     purpose_debt_consolidation + purpose_educational + purpose_home_improvement + 
##     purpose_house + purpose_major_purchase + purpose_medical + 
##     purpose_moving + purpose_renewable_energy + purpose_small_business + 
##     purpose_vacation + purpose_wedding + home_ownsh_x_mort_E + 
##     home_ownsh_x_own_E + home_ownsh_x_rent_E + home_ownsh_x_mort_F + 
##     home_ownsh_x_own_F + loan_amnt_x_term_36, family = binomial, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5132  -0.6108  -0.4707  -0.3131   2.9612  
## 
## Coefficients: (1 not defined because of singularities)
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                      -4.788e-01  9.401e-01  -0.509  0.61059    
## loan_amnt                        -1.144e-04  7.143e-05  -1.601  0.10927    
## installment                       4.722e-03  2.892e-03   1.633  0.10249    
## dti                               1.538e-02  6.345e-03   2.424  0.01536 *  
## annual_inc                       -2.128e-06  1.023e-06  -2.080  0.03750 *  
## term_months_36                   -4.787e-01  1.804e-01  -2.654  0.00795 ** 
## grade_a                          -1.089e+00  4.529e-01  -2.405  0.01616 *  
## grade_b                          -3.783e-01  4.226e-01  -0.895  0.37061    
## grade_c                          -2.950e-02  4.000e-01  -0.074  0.94122    
## grade_d                           1.270e-01  3.778e-01   0.336  0.73667    
## grade_e                           1.694e-02  3.963e-01   0.043  0.96590    
## grade_f                           1.968e-01  4.231e-01   0.465  0.64181    
## emp_length_2_years               -6.713e-02  1.114e-01  -0.603  0.54661    
## emp_length_2_5_years             -2.217e-01  9.223e-02  -2.404  0.01621 *  
## home_ownership_mortgage          -6.837e-01  8.298e-01  -0.824  0.40998    
## home_ownership_own               -1.967e-01  8.396e-01  -0.234  0.81476    
## home_ownership_rent              -4.486e-01  8.284e-01  -0.541  0.58820    
## verification_status_not_verified  1.562e-02  8.952e-02   0.174  0.86149    
## purpose_car                      -3.224e-01  2.626e-01  -1.228  0.21953    
## purpose_credit_card              -3.404e-01  1.857e-01  -1.833  0.06678 .  
## purpose_debt_consolidation       -9.780e-02  1.440e-01  -0.679  0.49689    
## purpose_educational               6.796e-01  3.830e-01   1.774  0.07600 .  
## purpose_home_improvement         -5.099e-02  2.037e-01  -0.250  0.80233    
## purpose_house                     3.472e-01  3.785e-01   0.917  0.35892    
## purpose_major_purchase           -4.344e-03  2.209e-01  -0.020  0.98431    
## purpose_medical                  -2.299e-01  3.711e-01  -0.619  0.53562    
## purpose_moving                    1.894e-01  3.215e-01   0.589  0.55580    
## purpose_renewable_energy          1.139e+00  6.503e-01   1.752  0.07976 .  
## purpose_small_business            7.183e-01  1.944e-01   3.695  0.00022 ***
## purpose_vacation                 -4.989e-01  5.450e-01  -0.915  0.35997    
## purpose_wedding                  -1.170e+00  3.895e-01  -3.004  0.00266 ** 
## home_ownsh_x_mort_E              -1.455e-01  2.813e-01  -0.517  0.60486    
## home_ownsh_x_own_E               -6.983e-03  5.173e-01  -0.013  0.98923    
## home_ownsh_x_rent_E                      NA         NA      NA       NA    
## home_ownsh_x_mort_F              -4.034e-01  4.130e-01  -0.977  0.32869    
## home_ownsh_x_own_F                6.257e-01  9.223e-01   0.678  0.49749    
## loan_amnt_x_term_36              -5.245e-05  2.865e-05  -1.831  0.06712 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4465.8  on 5384  degrees of freedom
## Residual deviance: 4134.2  on 5349  degrees of freedom
##   (615 observations deleted due to missingness)
## AIC: 4206.2
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>#use our trained model to predict default probabilities on the testing dataset
pred_on_test &lt;- predict(logistic_reg_trained, newdata = test, type = &quot;response&quot;)
pred_on_test_df &lt;- data.frame(pred_on_test)

#isolate the binary default probabilities of the testing data set to create the ROC curve
test_loan_stat &lt;- test[&quot;loan_stat_binary&quot;]

#create the ROC curve of our trained model predicting default probabilities on the testing dataset vs the actual defaults of the testing dataset
ROC_logreg_on_test &lt;- cbind(test_loan_stat,pred_on_test_df)
ROC_logreg_on_test_clean &lt;- na.omit(ROC_logreg_on_test)

PRROC_obj &lt;- roc.curve(scores.class0 = ROC_logreg_on_test_clean$pred_on_test, weights.class0=ROC_logreg_on_test_clean$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/training%20and%20testing%20datasets-1.png" width="672" /></p>
</div>
<div id="lets-exploit-the-model-to-select-loans-to-invest." class="section level2">
<h2>7. Let’s exploit the model to select loans to invest.</h2>
<div id="a.-before-we-focus-on-a-more-realistic-revenue-model" class="section level3">
<h3>a. Before we focus on a more realistic revenue model […]</h3>
<pre class="r"><code>#select actual defaults the from training set
training_loan_stat &lt;- train[&quot;loan_stat_binary&quot;]

#predict trained model on training dataset and put the prediction in a df
pred_on_trained &lt;- predict(logistic_reg_trained, newdata = train, type = &quot;response&quot;)
pred_on_trained_df &lt;- data.frame(pred_on_trained)

#create a first function to speed up the comparison, x being the cut-off, y the loss and x the profit
pred_on_trained_function &lt;- function(x,y,z){

  #transform the predicted default probabilities into binary default probabilities according to cut-off x
  pred_on_trained &lt;- ifelse(pred_on_trained_df &gt; x, 1, 0)

  #bind actual binary default probabilities of training set and predicted ones to later create our ROC curve
  pred_on_trained_vs_trained &lt;- cbind(training_loan_stat,pred_on_trained)

  #create our p&amp;l using a nested ifelse statement as follows:
  
  pred_on_trained_vs_trained$p_l &lt;- ifelse(pred_on_trained_vs_trained$loan_stat_binary == 1 &amp; pred_on_trained_vs_trained$pred_on_trained == 1,0,#if we predict the loan will default (1) and it defaults (1), then we did not invest and have a 0 p&amp;l
                                           ifelse(pred_on_trained_vs_trained$loan_stat_binary == 0 &amp; pred_on_trained_vs_trained$pred_on_trained == 1,0,#if we predict the loan will default (1) and it does not default (0), then we did not invest and have a 0 p&amp;l (though we incur a cost of opportunity)
                                                  ifelse(pred_on_trained_vs_trained$loan_stat_binary == 1 &amp; pred_on_trained_vs_trained$pred_on_trained == 0,y,#if we predict the loan will not default (0) and it defaults (1), then we invested and we lose y
                                                         ifelse(pred_on_trained_vs_trained$loan_stat_binary == 0 &amp; pred_on_trained_vs_trained$pred_on_trained == 0,z,z))))#if we predict the loan will not default (0) and it does not default (0), then we invested and we make a gain of x

  #clean our df with the p&amp;l from NAs      
  pred_on_trained_vs_trained_clean &lt;- na.omit(pred_on_trained_vs_trained)

  #print and paste the total p&amp;l
  pnl &lt;- sum(pred_on_trained_vs_trained_clean$p_l)
  print(paste(pnl))
}#end of first function

#create a second function using the first function to plot the graphs
graph_trained_on_train_function &lt;- function(x,y){

  #assign p&amp;ls for cut-offs by 0.5 increments
  pnl_05 &lt;- pred_on_trained_function(0.05,x,y)
  pnl_10 &lt;- pred_on_trained_function(0.1,x,y)
  pnl_15 &lt;- pred_on_trained_function(0.15,x,y)
  pnl_20 &lt;- pred_on_trained_function(0.2,x,y)
  pnl_25 &lt;- pred_on_trained_function(0.25,x,y)
  pnl_30 &lt;- pred_on_trained_function(0.3,x,y)
  pnl_35 &lt;- pred_on_trained_function(0.35,x,y)
  pnl_40 &lt;- pred_on_trained_function(0.40,x,y)
  pnl_45 &lt;- pred_on_trained_function(0.45,x,y)
  pnl_50 &lt;- pred_on_trained_function(0.50,x,y)
  pnl_55 &lt;- pred_on_trained_function(0.55,x,y)
  pnl_60 &lt;- pred_on_trained_function(0.60,x,y)
  pnl_65 &lt;- pred_on_trained_function(0.65,x,y)
  pnl_70 &lt;- pred_on_trained_function(0.70,x,y)
  pnl_75 &lt;- pred_on_trained_function(0.75,x,y)
  pnl_80 &lt;- pred_on_trained_function(0.80,x,y)
  pnl_85 &lt;- pred_on_trained_function(0.85,x,y)
  pnl_90 &lt;- pred_on_trained_function(0.90,x,y)
  pnl_95 &lt;- pred_on_trained_function(0.95,x,y)
  pnl_100 &lt;- pred_on_trained_function(1,x,y)

  #put our inputs in a df
  pred_on_trained_sensitivity &lt;- data.frame(cut_off = c(0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1),
                                            pnl = c(pnl_05,pnl_10,pnl_15,pnl_20,pnl_25,pnl_30,pnl_35,pnl_40,pnl_45,pnl_50,pnl_55,pnl_60,pnl_65,pnl_70,pnl_75,pnl_80,pnl_85,pnl_90,pnl_95,pnl_100))

  #set the p&amp;l df as numeric to have continuous scales on our graph
  pred_on_trained_sensitivity$cut_off &lt;- as.numeric(pred_on_trained_sensitivity$cut_off)
  pred_on_trained_sensitivity$pnl &lt;- as.numeric(pred_on_trained_sensitivity$pnl)

  #print the tables
  print(pred_on_trained_sensitivity)
  
  #plot the tables
  ggplot(pred_on_trained_sensitivity, aes(x=cut_off,y=pnl,group = 1))+
    geom_line(alpha=1) +
    theme_bw() +
    scale_y_continuous()+
    labs (
      title = paste(&quot;Sensitivity of P&amp;L to cut-off using the trained model on the training dataset\nwith loss =&quot;, x,&quot;and profit =&quot;,y),
      x     = &quot;Cut-off&quot;,
      y     = &quot;P&amp;L&quot;)
  }#end of function 2

#do a sensitivity analysis of profit by 10 increments
graph_trained_on_train_function(-100,20)</code></pre>
<pre><code>## [1] &quot;11440&quot;
## [1] &quot;24600&quot;
## [1] &quot;31460&quot;
## [1] &quot;29780&quot;
## [1] &quot;24820&quot;
## [1] &quot;20440&quot;
## [1] &quot;17420&quot;
## [1] &quot;15900&quot;
## [1] &quot;15580&quot;
## [1] &quot;14520&quot;
## [1] &quot;13880&quot;
## [1] &quot;13720&quot;
## [1] &quot;13720&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
## [1] &quot;13740&quot;
##    cut_off   pnl
## 1     0.05 11440
## 2     0.10 24600
## 3     0.15 31460
## 4     0.20 29780
## 5     0.25 24820
## 6     0.30 20440
## 7     0.35 17420
## 8     0.40 15900
## 9     0.45 15580
## 10    0.50 14520
## 11    0.55 13880
## 12    0.60 13720
## 13    0.65 13720
## 14    0.70 13740
## 15    0.75 13740
## 16    0.80 13740
## 17    0.85 13740
## 18    0.90 13740
## 19    0.95 13740
## 20    1.00 13740</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20trained-1.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_train_function(-100,30)</code></pre>
<pre><code>## [1] &quot;17960&quot;
## [1] &quot;42750&quot;
## [1] &quot;60640&quot;
## [1] &quot;66420&quot;
## [1] &quot;66330&quot;
## [1] &quot;64360&quot;
## [1] &quot;62430&quot;
## [1] &quot;61500&quot;
## [1] &quot;61420&quot;
## [1] &quot;60480&quot;
## [1] &quot;59870&quot;
## [1] &quot;59730&quot;
## [1] &quot;59730&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
## [1] &quot;59760&quot;
##    cut_off   pnl
## 1     0.05 17960
## 2     0.10 42750
## 3     0.15 60640
## 4     0.20 66420
## 5     0.25 66330
## 6     0.30 64360
## 7     0.35 62430
## 8     0.40 61500
## 9     0.45 61420
## 10    0.50 60480
## 11    0.55 59870
## 12    0.60 59730
## 13    0.65 59730
## 14    0.70 59760
## 15    0.75 59760
## 16    0.80 59760
## 17    0.85 59760
## 18    0.90 59760
## 19    0.95 59760
## 20    1.00 59760</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20trained-2.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_train_function(-100,40)</code></pre>
<pre><code>## [1] &quot;24480&quot;
## [1] &quot;60900&quot;
## [1] &quot;89820&quot;
## [1] &quot;103060&quot;
## [1] &quot;107840&quot;
## [1] &quot;108280&quot;
## [1] &quot;107440&quot;
## [1] &quot;107100&quot;
## [1] &quot;107260&quot;
## [1] &quot;106440&quot;
## [1] &quot;105860&quot;
## [1] &quot;105740&quot;
## [1] &quot;105740&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
## [1] &quot;105780&quot;
##    cut_off    pnl
## 1     0.05  24480
## 2     0.10  60900
## 3     0.15  89820
## 4     0.20 103060
## 5     0.25 107840
## 6     0.30 108280
## 7     0.35 107440
## 8     0.40 107100
## 9     0.45 107260
## 10    0.50 106440
## 11    0.55 105860
## 12    0.60 105740
## 13    0.65 105740
## 14    0.70 105780
## 15    0.75 105780
## 16    0.80 105780
## 17    0.85 105780
## 18    0.90 105780
## 19    0.95 105780
## 20    1.00 105780</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20trained-3.png" width="672" /></p>
<pre class="r"><code>#print the ROC curve
pred_on_trained_vs_trained &lt;- cbind(training_loan_stat,pred_on_trained)
pred_on_trained_vs_trained &lt;- na.omit(pred_on_trained_vs_trained)
PRROC_obj &lt;- roc.curve(scores.class0 = pred_on_trained_vs_trained$pred_on_trained, weights.class0=pred_on_trained_vs_trained$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20trained-4.png" width="672" /></p>
</div>
<div id="b.-investigate-whether-the-optimal-investment-cuttoff" class="section level3">
<h3>b. Investigate whether the optimal investment cuttoff […]</h3>
<pre class="r"><code>#select defaults from testing set
test_loan_stat &lt;- test[&quot;loan_stat_binary&quot;]

#predict trained model on testing set and put the prediction in a df
pred_on_test &lt;- predict(logistic_reg_trained, newdata = test, type = &quot;response&quot;)
pred_on_test_df &lt;- data.frame(pred_on_test)

#rest of the chunk is as above but using the testing set to predict default probabilities using our model trained on the traing set
pred_on_test_function &lt;- function(x,y,z){

  pred_on_test &lt;- ifelse(pred_on_test_df &gt; x, 1, 0)
  
  pred_on_test_vs_trained &lt;- cbind(test_loan_stat,pred_on_test)

  pred_on_test_vs_trained$p_l &lt;- ifelse(pred_on_test_vs_trained$loan_stat_binary == 1 &amp; pred_on_test_vs_trained$pred_on_test == 1,0,
                                           ifelse(pred_on_test_vs_trained$loan_stat_binary == 0 &amp; pred_on_test_vs_trained$pred_on_test == 1,0,
                                                  ifelse(pred_on_test_vs_trained$loan_stat_binary == 1 &amp; pred_on_test_vs_trained$pred_on_test == 0,y,
                                                         ifelse(pred_on_test_vs_trained$loan_stat_binary == 0 &amp; pred_on_test_vs_trained$pred_on_test == 0,z,z))))
                                                  
  pred_on_trained_vs_test_clean &lt;- na.omit(pred_on_test_vs_trained)

  pnl &lt;- sum(pred_on_trained_vs_test_clean$p_l)

  print(paste(pnl))
}

graph_trained_on_test_function &lt;- function(x,y){

  pnl_05 &lt;- pred_on_test_function(0.05,x,y)
  pnl_10 &lt;- pred_on_test_function(0.1,x,y)
  pnl_15 &lt;- pred_on_test_function(0.15,x,y)
  pnl_20 &lt;- pred_on_test_function(0.2,x,y)
  pnl_25 &lt;- pred_on_test_function(0.25,x,y)
  pnl_30 &lt;- pred_on_test_function(0.3,x,y)
  pnl_35 &lt;- pred_on_test_function(0.35,x,y)
  pnl_40 &lt;- pred_on_test_function(0.40,x,y)
  pnl_45 &lt;- pred_on_test_function(0.45,x,y)
  pnl_50 &lt;- pred_on_test_function(0.50,x,y)
  pnl_55 &lt;- pred_on_test_function(0.55,x,y)
  pnl_60 &lt;- pred_on_test_function(0.60,x,y)
  pnl_65 &lt;- pred_on_test_function(0.65,x,y)
  pnl_70 &lt;- pred_on_test_function(0.70,x,y)
  pnl_75 &lt;- pred_on_test_function(0.75,x,y)
  pnl_80 &lt;- pred_on_test_function(0.80,x,y)
  pnl_85 &lt;- pred_on_test_function(0.85,x,y)
  pnl_90 &lt;- pred_on_test_function(0.90,x,y)
  pnl_95 &lt;- pred_on_test_function(0.95,x,y)
  pnl_100 &lt;- pred_on_test_function(1,x,y)

  pred_on_test_sensitivity &lt;- data.frame(cut_off = c(0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1),
                                            pnl = c(pnl_05,pnl_10,pnl_15,pnl_20,pnl_25,pnl_30,pnl_35,pnl_40,pnl_45,pnl_50,pnl_55,pnl_60,pnl_65,pnl_70,pnl_75,pnl_80,pnl_85,pnl_90,pnl_95,pnl_100))

  pred_on_test_sensitivity$cut_off &lt;- as.numeric(pred_on_test_sensitivity$cut_off)
  pred_on_test_sensitivity$pnl &lt;- as.numeric(pred_on_test_sensitivity$pnl)

  print(pred_on_test_sensitivity)
  
  ggplot(pred_on_test_sensitivity, aes(x=cut_off,y=pnl,group = 1))+
    geom_line(alpha=1) +
    theme_bw() +
    scale_y_continuous()+
    labs (
      title = paste(&quot;Sensitivity of P&amp;L to cut-off using the trained model on the testing dataset\nwith loss =&quot;, x,&quot;and profit =&quot;,y),
      x     = &quot;Cut-off&quot;,
      y     = &quot;P&amp;L&quot;)
}

graph_trained_on_test_function(-100,20)</code></pre>
<pre><code>## [1] &quot;6920&quot;
## [1] &quot;16140&quot;
## [1] &quot;20980&quot;
## [1] &quot;20280&quot;
## [1] &quot;18920&quot;
## [1] &quot;16860&quot;
## [1] &quot;14800&quot;
## [1] &quot;13180&quot;
## [1] &quot;12580&quot;
## [1] &quot;12360&quot;
## [1] &quot;12500&quot;
## [1] &quot;12520&quot;
## [1] &quot;12540&quot;
## [1] &quot;12540&quot;
## [1] &quot;12560&quot;
## [1] &quot;12560&quot;
## [1] &quot;12560&quot;
## [1] &quot;12560&quot;
## [1] &quot;12560&quot;
## [1] &quot;12560&quot;
##    cut_off   pnl
## 1     0.05  6920
## 2     0.10 16140
## 3     0.15 20980
## 4     0.20 20280
## 5     0.25 18920
## 6     0.30 16860
## 7     0.35 14800
## 8     0.40 13180
## 9     0.45 12580
## 10    0.50 12360
## 11    0.55 12500
## 12    0.60 12520
## 13    0.65 12540
## 14    0.70 12540
## 15    0.75 12560
## 16    0.80 12560
## 17    0.85 12560
## 18    0.90 12560
## 19    0.95 12560
## 20    1.00 12560</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20test-1.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-100,30)</code></pre>
<pre><code>## [1] &quot;11280&quot;
## [1] &quot;27760&quot;
## [1] &quot;39670&quot;
## [1] &quot;44470&quot;
## [1] &quot;46480&quot;
## [1] &quot;46090&quot;
## [1] &quot;44950&quot;
## [1] &quot;43720&quot;
## [1] &quot;43270&quot;
## [1] &quot;43090&quot;
## [1] &quot;43300&quot;
## [1] &quot;43330&quot;
## [1] &quot;43360&quot;
## [1] &quot;43360&quot;
## [1] &quot;43390&quot;
## [1] &quot;43390&quot;
## [1] &quot;43390&quot;
## [1] &quot;43390&quot;
## [1] &quot;43390&quot;
## [1] &quot;43390&quot;
##    cut_off   pnl
## 1     0.05 11280
## 2     0.10 27760
## 3     0.15 39670
## 4     0.20 44470
## 5     0.25 46480
## 6     0.30 46090
## 7     0.35 44950
## 8     0.40 43720
## 9     0.45 43270
## 10    0.50 43090
## 11    0.55 43300
## 12    0.60 43330
## 13    0.65 43360
## 14    0.70 43360
## 15    0.75 43390
## 16    0.80 43390
## 17    0.85 43390
## 18    0.90 43390
## 19    0.95 43390
## 20    1.00 43390</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20test-2.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-100,40)</code></pre>
<pre><code>## [1] &quot;15640&quot;
## [1] &quot;39380&quot;
## [1] &quot;58360&quot;
## [1] &quot;68660&quot;
## [1] &quot;74040&quot;
## [1] &quot;75320&quot;
## [1] &quot;75100&quot;
## [1] &quot;74260&quot;
## [1] &quot;73960&quot;
## [1] &quot;73820&quot;
## [1] &quot;74100&quot;
## [1] &quot;74140&quot;
## [1] &quot;74180&quot;
## [1] &quot;74180&quot;
## [1] &quot;74220&quot;
## [1] &quot;74220&quot;
## [1] &quot;74220&quot;
## [1] &quot;74220&quot;
## [1] &quot;74220&quot;
## [1] &quot;74220&quot;
##    cut_off   pnl
## 1     0.05 15640
## 2     0.10 39380
## 3     0.15 58360
## 4     0.20 68660
## 5     0.25 74040
## 6     0.30 75320
## 7     0.35 75100
## 8     0.40 74260
## 9     0.45 73960
## 10    0.50 73820
## 11    0.55 74100
## 12    0.60 74140
## 13    0.65 74180
## 14    0.70 74180
## 15    0.75 74220
## 16    0.80 74220
## 17    0.85 74220
## 18    0.90 74220
## 19    0.95 74220
## 20    1.00 74220</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20test-3.png" width="672" /></p>
<pre class="r"><code>pred_on_test_vs_trained &lt;- cbind(test_loan_stat,pred_on_test)
pred_on_test_vs_trained &lt;- na.omit(pred_on_test_vs_trained)

PRROC_obj &lt;- roc.curve(scores.class0 = pred_on_test_vs_trained$pred_on_test, weights.class0=pred_on_test_vs_trained$loan_stat_binary,
                       curve=TRUE)
plot(PRROC_obj)</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensitivity%20trained%20on%20test-4.png" width="672" /></p>
</div>
<div id="c.-lets-stay-with-the-validation-dataset-which-has-4000-loans.-lets-build-a-d.-investigate-how-sensitive-is-the-performance-of-your" class="section level3">
<h3>c. Let’s stay with the validation dataset (which has 4,000 loans). Let’s build a […] &amp; d. Investigate how sensitive is the performance of your […]</h3>
<pre class="r"><code>#select all the terms used to compute the new return
test_loan_excerpt &lt;- test[c(&quot;term_months&quot;,&quot;loan_amnt&quot;,&quot;installment&quot;,&quot;loan_stat_binary&quot;)]

#compute the return per loan if no default
test_loan_excerpt$return &lt;- ((test_loan_excerpt$installment * test_loan_excerpt$term_months)/test_loan_excerpt$loan_amnt)-1

#as in the two chunks above
pred_on_test_new &lt;- predict(logistic_reg_trained, newdata = test, type = &quot;response&quot;)
pred_on_test_new_df &lt;- data.frame(pred_on_test_new)

#create a first function to speed up the comparison, x being the cut-off and y the loss in terms of return (note: z is removed since the return is determined by the formula created above)
pred_on_test_new_function &lt;- function(x,y){

  pred_on_test_new &lt;- ifelse(pred_on_test_new_df &gt; x, 1, 0)

  pred_on_test_vs_trained_new &lt;- cbind(test_loan_excerpt,pred_on_test_new)
  pred_on_test_vs_trained_new$roi &lt;- ifelse(pred_on_test_vs_trained_new$loan_stat_binary == 1 &amp; pred_on_test_vs_trained_new$pred_on_test_new == 1,0,
                                           ifelse(pred_on_test_vs_trained_new$loan_stat_binary == 0 &amp; pred_on_test_vs_trained_new$pred_on_test_new == 1,0,
                                                  ifelse(pred_on_test_vs_trained_new$loan_stat_binary == 1 &amp; pred_on_test_vs_trained_new$pred_on_test_new == 0,y,
                                                         ifelse(pred_on_test_vs_trained_new$loan_stat_binary == 0 &amp; pred_on_test_vs_trained_new$pred_on_test_new == 0,pred_on_test_vs_trained_new$return,pred_on_test_vs_trained_new$return))))#now using the formula for returns for loans we invested in that did not default
  
  #clean the data                                       
  pred_on_test_vs_trained_new_clean &lt;- na.omit(pred_on_test_vs_trained_new)

  #we invest one dollar in each loan, so the total return is simply going to be the sum of the returns
  pnl &lt;- sum(pred_on_test_vs_trained_new_clean$roi)

  print(paste(pnl))
}

#rest is as in the two chunk above
graph_trained_on_test_function &lt;- function(x){

  pnl_05 &lt;- pred_on_test_new_function(0.05,x)
  pnl_10 &lt;- pred_on_test_new_function(0.1,x)
  pnl_15 &lt;- pred_on_test_new_function(0.15,x)
  pnl_20 &lt;- pred_on_test_new_function(0.2,x)
  pnl_25 &lt;- pred_on_test_new_function(0.25,x)
  pnl_30 &lt;- pred_on_test_new_function(0.3,x)
  pnl_35 &lt;- pred_on_test_new_function(0.35,x)
  pnl_40 &lt;- pred_on_test_new_function(0.40,x)
  pnl_45 &lt;- pred_on_test_new_function(0.45,x)
  pnl_50 &lt;- pred_on_test_new_function(0.50,x)
  pnl_55 &lt;- pred_on_test_new_function(0.55,x)
  pnl_60 &lt;- pred_on_test_new_function(0.60,x)
  pnl_65 &lt;- pred_on_test_new_function(0.65,x)
  pnl_70 &lt;- pred_on_test_new_function(0.70,x)
  pnl_75 &lt;- pred_on_test_new_function(0.75,x)
  pnl_80 &lt;- pred_on_test_new_function(0.80,x)
  pnl_85 &lt;- pred_on_test_new_function(0.85,x)
  pnl_90 &lt;- pred_on_test_new_function(0.90,x)
  pnl_95 &lt;- pred_on_test_new_function(0.95,x)
  pnl_100 &lt;- pred_on_test_new_function(1,x)

  pred_on_test_new_sensitivity &lt;- data.frame(cut_off = c(0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1),
                                            pnl = c(pnl_05,pnl_10,pnl_15,pnl_20,pnl_25,pnl_30,pnl_35,pnl_40,pnl_45,pnl_50,pnl_55,pnl_60,pnl_65,pnl_70,pnl_75,pnl_80,pnl_85,pnl_90,pnl_95,pnl_100))

  pred_on_test_new_sensitivity$cut_off &lt;- as.numeric(pred_on_test_new_sensitivity$cut_off)
  pred_on_test_new_sensitivity$pnl &lt;- as.numeric(pred_on_test_new_sensitivity$pnl)
  
  print(pred_on_test_new_sensitivity)

  ggplot(pred_on_test_new_sensitivity, aes(x=cut_off,y=pnl,group = 1))+
    geom_line(alpha=1) +
    theme_bw() +
    labs (
      title = paste(&quot;Sensitivity of P&amp;L to cut-off using the trained model on the testing dataset\nwith loss =&quot;, x),
      x     = &quot;Cut-off&quot;,
      y     = &quot;P&amp;L&quot;)
}

#print the output graphs for different loss rate by 0.25 increments
graph_trained_on_test_function(-0)</code></pre>
<pre><code>## [1] &quot;53.6346427316225&quot;
## [1] &quot;166.059507896717&quot;
## [1] &quot;318.943486000972&quot;
## [1] &quot;467.593723032735&quot;
## [1] &quot;591.324441148302&quot;
## [1] &quot;658.65668751041&quot;
## [1] &quot;697.429341051245&quot;
## [1] &quot;714.811046648356&quot;
## [1] &quot;721.38139666406&quot;
## [1] &quot;723.209470758353&quot;
## [1] &quot;726.381435731615&quot;
## [1] &quot;726.925355731615&quot;
## [1] &quot;727.435305731615&quot;
## [1] &quot;727.435305731615&quot;
## [1] &quot;727.991362874472&quot;
## [1] &quot;727.991362874472&quot;
## [1] &quot;727.991362874472&quot;
## [1] &quot;727.991362874472&quot;
## [1] &quot;727.991362874472&quot;
## [1] &quot;727.991362874472&quot;
##    cut_off       pnl
## 1     0.05  53.63464
## 2     0.10 166.05951
## 3     0.15 318.94349
## 4     0.20 467.59372
## 5     0.25 591.32444
## 6     0.30 658.65669
## 7     0.35 697.42934
## 8     0.40 714.81105
## 9     0.45 721.38140
## 10    0.50 723.20947
## 11    0.55 726.38144
## 12    0.60 726.92536
## 13    0.65 727.43531
## 14    0.70 727.43531
## 15    0.75 727.99136
## 16    0.80 727.99136
## 17    0.85 727.99136
## 18    0.90 727.99136
## 19    0.95 727.99136
## 20    1.00 727.99136</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensivity%20trained%20on%20test%20with%20realistic%20return-1.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-0.25)</code></pre>
<pre><code>## [1] &quot;49.1346427316225&quot;
## [1] &quot;148.309507896717&quot;
## [1] &quot;277.943486000972&quot;
## [1] &quot;397.343723032735&quot;
## [1] &quot;500.824441148302&quot;
## [1] &quot;554.65668751041&quot;
## [1] &quot;583.679341051245&quot;
## [1] &quot;595.061046648356&quot;
## [1] &quot;599.38139666406&quot;
## [1] &quot;600.459470758353&quot;
## [1] &quot;603.631435731615&quot;
## [1] &quot;604.175355731615&quot;
## [1] &quot;604.685305731615&quot;
## [1] &quot;604.685305731615&quot;
## [1] &quot;605.241362874472&quot;
## [1] &quot;605.241362874472&quot;
## [1] &quot;605.241362874472&quot;
## [1] &quot;605.241362874472&quot;
## [1] &quot;605.241362874472&quot;
## [1] &quot;605.241362874472&quot;
##    cut_off       pnl
## 1     0.05  49.13464
## 2     0.10 148.30951
## 3     0.15 277.94349
## 4     0.20 397.34372
## 5     0.25 500.82444
## 6     0.30 554.65669
## 7     0.35 583.67934
## 8     0.40 595.06105
## 9     0.45 599.38140
## 10    0.50 600.45947
## 11    0.55 603.63144
## 12    0.60 604.17536
## 13    0.65 604.68531
## 14    0.70 604.68531
## 15    0.75 605.24136
## 16    0.80 605.24136
## 17    0.85 605.24136
## 18    0.90 605.24136
## 19    0.95 605.24136
## 20    1.00 605.24136</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensivity%20trained%20on%20test%20with%20realistic%20return-2.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-0.5)</code></pre>
<pre><code>## [1] &quot;44.6346427316225&quot;
## [1] &quot;130.559507896717&quot;
## [1] &quot;236.943486000972&quot;
## [1] &quot;327.093723032735&quot;
## [1] &quot;410.324441148302&quot;
## [1] &quot;450.65668751041&quot;
## [1] &quot;469.929341051245&quot;
## [1] &quot;475.311046648356&quot;
## [1] &quot;477.38139666406&quot;
## [1] &quot;477.709470758353&quot;
## [1] &quot;480.881435731615&quot;
## [1] &quot;481.425355731615&quot;
## [1] &quot;481.935305731615&quot;
## [1] &quot;481.935305731615&quot;
## [1] &quot;482.491362874472&quot;
## [1] &quot;482.491362874472&quot;
## [1] &quot;482.491362874472&quot;
## [1] &quot;482.491362874472&quot;
## [1] &quot;482.491362874472&quot;
## [1] &quot;482.491362874472&quot;
##    cut_off       pnl
## 1     0.05  44.63464
## 2     0.10 130.55951
## 3     0.15 236.94349
## 4     0.20 327.09372
## 5     0.25 410.32444
## 6     0.30 450.65669
## 7     0.35 469.92934
## 8     0.40 475.31105
## 9     0.45 477.38140
## 10    0.50 477.70947
## 11    0.55 480.88144
## 12    0.60 481.42536
## 13    0.65 481.93531
## 14    0.70 481.93531
## 15    0.75 482.49136
## 16    0.80 482.49136
## 17    0.85 482.49136
## 18    0.90 482.49136
## 19    0.95 482.49136
## 20    1.00 482.49136</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensivity%20trained%20on%20test%20with%20realistic%20return-3.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-0.75)</code></pre>
<pre><code>## [1] &quot;40.1346427316225&quot;
## [1] &quot;112.809507896717&quot;
## [1] &quot;195.943486000972&quot;
## [1] &quot;256.843723032735&quot;
## [1] &quot;319.824441148302&quot;
## [1] &quot;346.65668751041&quot;
## [1] &quot;356.179341051245&quot;
## [1] &quot;355.561046648356&quot;
## [1] &quot;355.38139666406&quot;
## [1] &quot;354.959470758353&quot;
## [1] &quot;358.131435731615&quot;
## [1] &quot;358.675355731615&quot;
## [1] &quot;359.185305731615&quot;
## [1] &quot;359.185305731615&quot;
## [1] &quot;359.741362874472&quot;
## [1] &quot;359.741362874472&quot;
## [1] &quot;359.741362874472&quot;
## [1] &quot;359.741362874472&quot;
## [1] &quot;359.741362874472&quot;
## [1] &quot;359.741362874472&quot;
##    cut_off       pnl
## 1     0.05  40.13464
## 2     0.10 112.80951
## 3     0.15 195.94349
## 4     0.20 256.84372
## 5     0.25 319.82444
## 6     0.30 346.65669
## 7     0.35 356.17934
## 8     0.40 355.56105
## 9     0.45 355.38140
## 10    0.50 354.95947
## 11    0.55 358.13144
## 12    0.60 358.67536
## 13    0.65 359.18531
## 14    0.70 359.18531
## 15    0.75 359.74136
## 16    0.80 359.74136
## 17    0.85 359.74136
## 18    0.90 359.74136
## 19    0.95 359.74136
## 20    1.00 359.74136</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensivity%20trained%20on%20test%20with%20realistic%20return-4.png" width="672" /></p>
<pre class="r"><code>graph_trained_on_test_function(-1)</code></pre>
<pre><code>## [1] &quot;35.6346427316225&quot;
## [1] &quot;95.0595078967165&quot;
## [1] &quot;154.943486000972&quot;
## [1] &quot;186.593723032735&quot;
## [1] &quot;229.324441148302&quot;
## [1] &quot;242.65668751041&quot;
## [1] &quot;242.429341051245&quot;
## [1] &quot;235.811046648356&quot;
## [1] &quot;233.38139666406&quot;
## [1] &quot;232.209470758353&quot;
## [1] &quot;235.381435731615&quot;
## [1] &quot;235.925355731615&quot;
## [1] &quot;236.435305731615&quot;
## [1] &quot;236.435305731615&quot;
## [1] &quot;236.991362874472&quot;
## [1] &quot;236.991362874472&quot;
## [1] &quot;236.991362874472&quot;
## [1] &quot;236.991362874472&quot;
## [1] &quot;236.991362874472&quot;
## [1] &quot;236.991362874472&quot;
##    cut_off       pnl
## 1     0.05  35.63464
## 2     0.10  95.05951
## 3     0.15 154.94349
## 4     0.20 186.59372
## 5     0.25 229.32444
## 6     0.30 242.65669
## 7     0.35 242.42934
## 8     0.40 235.81105
## 9     0.45 233.38140
## 10    0.50 232.20947
## 11    0.55 235.38144
## 12    0.60 235.92536
## 13    0.65 236.43531
## 14    0.70 236.43531
## 15    0.75 236.99136
## 16    0.80 236.99136
## 17    0.85 236.99136
## 18    0.90 236.99136
## 19    0.95 236.99136
## 20    1.00 236.99136</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/sensivity%20trained%20on%20test%20with%20realistic%20return-5.png" width="672" /></p>
</div>
</div>
</div>
<div id="assessment-2" class="section level1">
<h1>Assessment 2</h1>
<div id="format-the-assessment-2-dataset" class="section level2">
<h2>1. Format the assessment 2 dataset</h2>
<pre class="r"><code>#read the assessment data
assessment_data &lt;- read_csv((here::here(&quot;assessment_raw_data.csv&quot;)))

#recode the categorical variables as we did for the workshop dataset
assessment_data$emp_length &lt;- recode(assessment_data$emp_length, &#39;&lt; 1 year&#39; = &#39;&lt; 2 years&#39;, &#39;1 year&#39; = &#39;&lt; 2 years&#39;, &#39;2 years&#39; = &#39;2-5 years&#39;, &#39;3 years&#39; = &#39;2-5 years&#39;, &#39;4 years&#39; = &#39;2-5 years&#39;, &#39;5 years&#39; = &#39;2-5 years&#39;, &#39;6 years&#39; = &#39;&gt; 5 years&#39;,&#39;7 years&#39; = &#39;&gt; 5 years&#39;,&#39;8 years&#39; = &#39;&gt; 5 years&#39;,&#39;9 years&#39; = &#39;&gt; 5 years&#39;,&#39;10+ years&#39; = &#39;&gt; 5 years&#39;)
assessment_data$delinq_2yrs &lt;- recode(assessment_data$delinq_2yrs, &#39;0&#39; = &#39;&lt; 2&#39;, &#39;1&#39; = &#39;&lt; 2&#39;, &#39;2&#39; = &#39;2-5&#39;, &#39;3&#39; = &#39;2-5&#39;, &#39;4&#39; = &#39;2-5&#39;, &#39;5&#39; = &#39;2-5&#39;, &#39;6&#39; = &#39;&gt; 5&#39;,&#39;7&#39; = &#39;&gt; 5&#39;,&#39;8&#39; = &#39;&gt; 5&#39;,&#39;9&#39; = &#39;&gt; 5&#39;)
assessment_data$verification_status &lt;- recode(assessment_data$verification_status, &#39;Source Verified&#39; = &#39;Verified&#39;, &#39;Verified&#39; = &#39;Verified&#39;, &#39;Not Verified&#39; = &#39;Not Verified&#39;)

#create the same dummies as in the workshop dataset
assessment_data_dummied &lt;- dummy_cols(assessment_data, select_columns = c(&#39;term (months)&#39;,&#39;grade&#39;,&#39;emp_length&#39;,&#39;home_ownership&#39;,&#39;verification_status&#39;))

#delete the useless dummies
drops &lt;- c(&quot;term (months)_60&quot;,&quot;term (months)_NA&quot;,&quot;grade_G&quot;,&quot;grade_NA&quot;,&quot;emp_length_&gt; 5 years&quot;,&quot;emp_length_n/a&quot;,&quot;emp_length_NA&quot;,&quot;home_ownership_OTHER&quot;,&quot;home_ownership_NA&quot;,&quot;verification_status_Verified&quot;,&quot;verification_status_NA&quot;)
assessment_data_dummied_cleaned &lt;- assessment_data_dummied[ , !(names(assessment_data_dummied) %in% drops)]
assessment_data_dummied_cleaned_names &lt;- clean_names(assessment_data_dummied_cleaned)

#add our interaction terms of low grades * home ownership
assessment_data_dummied_cleaned_names$home_ownsh_x_mort_E &lt;- assessment_data_dummied_cleaned_names$home_ownership_mortgage * assessment_data_dummied_cleaned_names$grade_e
assessment_data_dummied_cleaned_names$home_ownsh_x_own_E &lt;- assessment_data_dummied_cleaned_names$home_ownership_own * assessment_data_dummied_cleaned_names$grade_e
assessment_data_dummied_cleaned_names$home_ownsh_x_rent_E &lt;- assessment_data_dummied_cleaned_names$home_ownership_rent * assessment_data_dummied_cleaned_names$grade_e
assessment_data_dummied_cleaned_names$home_ownsh_x_mort_F &lt;- assessment_data_dummied_cleaned_names$home_ownership_mortgage * assessment_data_dummied_cleaned_names$grade_f
assessment_data_dummied_cleaned_names$home_ownsh_x_own_F &lt;- assessment_data_dummied_cleaned_names$home_ownership_own * assessment_data_dummied_cleaned_names$grade_f
assessment_data_dummied_cleaned_names$home_ownsh_x_rent_F &lt;- assessment_data_dummied_cleaned_names$home_ownership_rent * assessment_data_dummied_cleaned_names$grade_f

#add our interaction term of loan term * loan amount
assessment_data_dummied_cleaned_names$loan_amnt_x_term_36 &lt;- assessment_data_dummied_cleaned_names$loan_amnt * assessment_data_dummied_cleaned_names$term_months_36

colnames(assessment_data_dummied_cleaned_names)</code></pre>
<pre><code>##  [1] &quot;loan_number&quot;                      &quot;int_rate&quot;                        
##  [3] &quot;loan_amnt&quot;                        &quot;term_months&quot;                     
##  [5] &quot;installment&quot;                      &quot;dti&quot;                             
##  [7] &quot;delinq_2yrs&quot;                      &quot;annual_inc&quot;                      
##  [9] &quot;grade&quot;                            &quot;emp_title&quot;                       
## [11] &quot;emp_length&quot;                       &quot;home_ownership&quot;                  
## [13] &quot;verification_status&quot;              &quot;issue_d&quot;                         
## [15] &quot;invest&quot;                           &quot;term_months_36&quot;                  
## [17] &quot;grade_a&quot;                          &quot;grade_b&quot;                         
## [19] &quot;grade_c&quot;                          &quot;grade_d&quot;                         
## [21] &quot;grade_e&quot;                          &quot;grade_f&quot;                         
## [23] &quot;emp_length_2_years&quot;               &quot;emp_length_2_5_years&quot;            
## [25] &quot;home_ownership_mortgage&quot;          &quot;home_ownership_own&quot;              
## [27] &quot;home_ownership_rent&quot;              &quot;verification_status_not_verified&quot;
## [29] &quot;home_ownsh_x_mort_E&quot;              &quot;home_ownsh_x_own_E&quot;              
## [31] &quot;home_ownsh_x_rent_E&quot;              &quot;home_ownsh_x_mort_F&quot;             
## [33] &quot;home_ownsh_x_own_F&quot;               &quot;home_ownsh_x_rent_F&quot;             
## [35] &quot;loan_amnt_x_term_36&quot;</code></pre>
</div>
<div id="fit-our-best-model-on-the-formatted-assessment-2-data-set" class="section level2">
<h2>2. Fit our best model on the formatted assessment 2 data set</h2>
<pre class="r"><code>#train our latest model on the lending club sample used in the workshop (note: we did not use the purpose variables since informatiom about purposes was not available)
logistic_reg_assessment_data &lt;- glm(loan_stat_binary ~ 
                        loan_amnt + 
                        installment + 
                        dti +
                        annual_inc +
                        term_months_36 + 
                        grade_a + grade_b + grade_c + grade_d + grade_e + grade_f + 
                        emp_length_2_years + emp_length_2_5_years + 
                        home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
                        verification_status_not_verified + 
                        home_ownsh_x_mort_E + home_ownsh_x_own_E + home_ownsh_x_rent_E +
                        home_ownsh_x_mort_F + home_ownsh_x_own_F +
                        loan_amnt_x_term_36,
                        family = binomial,
                        data = lendingclub_sample_dummied_cleaned_names)
summary(logistic_reg_assessment_data)</code></pre>
<pre><code>## 
## Call:
## glm(formula = loan_stat_binary ~ loan_amnt + installment + dti + 
##     annual_inc + term_months_36 + grade_a + grade_b + grade_c + 
##     grade_d + grade_e + grade_f + emp_length_2_years + emp_length_2_5_years + 
##     home_ownership_mortgage + home_ownership_own + home_ownership_rent + 
##     verification_status_not_verified + home_ownsh_x_mort_E + 
##     home_ownsh_x_own_E + home_ownsh_x_rent_E + home_ownsh_x_mort_F + 
##     home_ownsh_x_own_F + loan_amnt_x_term_36, family = binomial, 
##     data = lendingclub_sample_dummied_cleaned_names)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1854  -0.6004  -0.4789  -0.3260   3.5903  
## 
## Coefficients: (1 not defined because of singularities)
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                      -1.897e-01  7.213e-01  -0.263  0.79252    
## loan_amnt                        -1.625e-04  5.582e-05  -2.912  0.00360 ** 
## installment                       6.631e-03  2.263e-03   2.930  0.00339 ** 
## dti                               8.813e-03  4.812e-03   1.831  0.06703 .  
## annual_inc                       -4.284e-06  9.747e-07  -4.395 1.11e-05 ***
## term_months_36                   -6.749e-01  1.373e-01  -4.915 8.86e-07 ***
## grade_a                          -1.071e+00  3.566e-01  -3.003  0.00268 ** 
## grade_b                          -4.190e-01  3.334e-01  -1.257  0.20892    
## grade_c                          -1.272e-01  3.163e-01  -0.402  0.68744    
## grade_d                          -6.667e-02  2.995e-01  -0.223  0.82384    
## grade_e                          -2.456e-01  3.112e-01  -0.789  0.42991    
## grade_f                           8.220e-02  3.338e-01   0.246  0.80548    
## emp_length_2_years               -7.027e-03  8.550e-02  -0.082  0.93450    
## emp_length_2_5_years             -1.972e-01  7.193e-02  -2.742  0.00610 ** 
## home_ownership_mortgage          -5.825e-01  6.445e-01  -0.904  0.36613    
## home_ownership_own               -2.260e-01  6.519e-01  -0.347  0.72885    
## home_ownership_rent              -3.977e-01  6.431e-01  -0.618  0.53628    
## verification_status_not_verified -4.536e-02  6.946e-02  -0.653  0.51375    
## home_ownsh_x_mort_E              -1.085e-01  2.227e-01  -0.488  0.62590    
## home_ownsh_x_own_E               -6.166e-02  4.014e-01  -0.154  0.87793    
## home_ownsh_x_rent_E                      NA         NA      NA       NA    
## home_ownsh_x_mort_F              -2.050e-01  3.060e-01  -0.670  0.50291    
## home_ownsh_x_own_F                2.621e-01  7.177e-01   0.365  0.71494    
## loan_amnt_x_term_36              -6.599e-05  2.233e-05  -2.955  0.00312 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7327.4  on 8958  degrees of freedom
## Residual deviance: 6861.2  on 8936  degrees of freedom
##   (1041 observations deleted due to missingness)
## AIC: 6907.2
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>#use this model to predict the default probabilities on the assessment data set and store in a df
pred_logistic_reg_assessment_data &lt;- predict(logistic_reg_assessment_data, newdata = assessment_data_dummied_cleaned_names, type = &quot;response&quot;)
pred_logistic_reg_assessment_data_df &lt;- data.frame(pred_logistic_reg_assessment_data)

#add a column in which the return per loan if no default is computed
assessment_data_dummied_cleaned_names_excerpt &lt;- assessment_data_dummied_cleaned_names[c(&quot;term_months&quot;,&quot;loan_amnt&quot;,&quot;installment&quot;)]
assessment_data_dummied_cleaned_names_excerpt$return_no_def &lt;- ((assessment_data_dummied_cleaned_names$installment * assessment_data_dummied_cleaned_names$term_months)/assessment_data_dummied_cleaned_names$loan_amnt)-1

#create a df with the loan number, the predicted default probability and the return if no default
pred_logistic_reg_assessment_data_df_complete &lt;- cbind(assessment_data_dummied_cleaned_names$loan_number,pred_logistic_reg_assessment_data_df,assessment_data_dummied_cleaned_names_excerpt$return_no_def)

#rename the columns with shorter names
names(pred_logistic_reg_assessment_data_df_complete)[1] &lt;- &quot;loan_nb&quot;
names(pred_logistic_reg_assessment_data_df_complete)[2] &lt;- &quot;predicted_default_prob&quot;
names(pred_logistic_reg_assessment_data_df_complete)[3] &lt;- &quot;return_no_default&quot;

#plot the graph of return if no default vs default probability to assess the consistency of our model visually (we would expect higher default probability as the return increases)
ggplot(pred_logistic_reg_assessment_data_df_complete, aes(x=predicted_default_prob,y=return_no_default,group = 1))+
  geom_point(alpha=1) +
  theme_bw() +
  scale_y_continuous()+
  labs (
    title = paste(&quot;Probability of default VS return if no default&quot;),
    x     = &quot;Probability of default&quot;,
    y     = &quot;Return if no default&quot;) #changing y-axis label to sentence case</code></pre>
<p><img src="/Hadrien_Pistre_Assessment_2_R_Code_files/figure-html/using%20best%20model%20on%20assessment%20data-1.png" width="672" /></p>
</div>
<div id="use-our-best-model-to-compute-the-pl-on-the-assessment-2-dataset" class="section level2">
<h2>3. Use our best model to compute the P&amp;L on the assessment 2 dataset</h2>
<pre class="r"><code>#create a column with binary predicted default values based on the cut7-off
pred_logistic_reg_assessment_data_df_complete$predicted_default_binary &lt;- ifelse(pred_logistic_reg_assessment_data_df_complete$predicted_default_prob &gt; 0.2, 1, 0)

#create a pnl column replacing the binary default values by their pnl
pred_logistic_reg_assessment_data_df_complete$pnl &lt;- ifelse(pred_logistic_reg_assessment_data_df_complete$predicted_default_binary == 1,-0.5,pred_logistic_reg_assessment_data_df_complete$return_no_default)

#clean the obtained dataframe                                                  
pred_logistic_reg_assessment_data_df_complete &lt;- na.omit(pred_logistic_reg_assessment_data_df_complete)

#create a new column with another selection criterion to select the loans: the inverse of the default probability * the return -&gt; hence the loans with the highest score will be those with the lowest default probability and the highest return
pred_logistic_reg_assessment_data_df_complete$criterion &lt;- (1/pred_logistic_reg_assessment_data_df_complete$predicted_default_prob) * (pred_logistic_reg_assessment_data_df_complete$return_no_default)
  
#arrange the table by pnl using the predicted returns by our model
pred_logistic_reg_assessment_data_df_complete_ordered_by_pnl &lt;- arrange(pred_logistic_reg_assessment_data_df_complete,desc(pnl))
  
#print the first 200 loans by pnl which would be the ones we would invest in
top_200_pnl &lt;- head(pred_logistic_reg_assessment_data_df_complete_ordered_by_pnl,200)
  by_pnl_total_pnl &lt;- sum(top_200_pnl$pnl)
  by_pnl_total_pnl</code></pre>
<pre><code>## [1] 65.95934</code></pre>
<pre class="r"><code>print(top_200_pnl)</code></pre>
<pre><code>##      loan_nb predicted_default_prob return_no_default predicted_default_binary
## 1334    1334           1.234157e-01         0.6233333                        0
## 715      715           1.409516e-01         0.6117647                        0
## 182      182           1.892754e-01         0.5840000                        0
## 864      864           1.927243e-01         0.5800000                        0
## 1739    1739           1.918502e-01         0.5650000                        0
## 972      972           1.934637e-01         0.5491943                        0
## 1722    1722           1.940004e-01         0.5428571                        0
## 1331    1331           1.898454e-01         0.5360000                        0
## 978      978           1.640918e-01         0.5240000                        0
## 208      208           1.918976e-01         0.5200000                        0
## 1647    1647           1.788912e-01         0.5094737                        0
## 247      247           1.865956e-01         0.5048000                        0
## 1016    1016           1.941021e-01         0.5048000                        0
## 65        65           3.137086e-02         0.5000000                        0
## 146      146           1.824092e-01         0.4880000                        0
## 259      259           1.825288e-01         0.4863636                        0
## 1612    1612           1.796668e-01         0.4820000                        0
## 480      480           1.712898e-01         0.4447368                        0
## 1765    1765           1.960274e-01         0.4357143                        0
## 465      465           1.748325e-01         0.4352000                        0
## 378      378           1.999145e-01         0.4350000                        0
## 1738    1738           1.741319e-01         0.4333333                        0
## 739      739           1.835169e-01         0.4200000                        0
## 20        20           1.951413e-01         0.4160000                        0
## 263      263           1.363783e-01         0.3944000                        0
## 418      418           1.316926e-01         0.3896000                        0
## 266      266           1.089148e-01         0.3848000                        0
## 729      729           1.773141e-01         0.3800000                        0
## 1148    1148           3.959997e-02         0.3800000                        0
## 1567    1567           1.597954e-01         0.3800000                        0
## 1514    1514           1.975660e-01         0.3776000                        0
## 467      467           1.665930e-01         0.3728000                        0
## 1220    1220           1.601605e-01         0.3728000                        0
## 168      168           1.798173e-01         0.3656000                        0
## 867      867           1.687222e-01         0.3645714                        0
## 1500    1500           1.539495e-01         0.3640000                        0
## 10        10           1.735487e-01         0.3565217                        0
## 341      341           8.814542e-02         0.3560000                        0
## 1608    1608           1.831778e-01         0.3560000                        0
## 123      123           1.723526e-01         0.3550000                        0
## 7          7           1.964117e-01         0.3545455                        0
## 416      416           1.875745e-01         0.3538462                        0
## 767      767           1.918474e-01         0.3521951                        0
## 354      354           1.531520e-01         0.3480000                        0
## 929      929           9.036943e-02         0.3474286                        0
## 1295    1295           1.479463e-01         0.3470000                        0
## 118      118           1.595559e-01         0.3450000                        0
## 1105    1105           1.795950e-01         0.3432836                        0
## 714      714           1.697544e-01         0.3360825                        0
## 1350    1350           1.288027e-01         0.3350000                        0
## 717      717           1.574248e-01         0.3336634                        0
## 179      179           1.527065e-01         0.3328571                        0
## 30        30           1.223897e-01         0.3320000                        0
## 256      256           1.654141e-01         0.3300000                        0
## 1024    1024           1.062149e-01         0.3296000                        0
## 1646    1646           1.340497e-01         0.3296000                        0
## 1294    1294           1.746048e-01         0.3284000                        0
## 1761    1761           1.894630e-01         0.3283721                        0
## 832      832           1.477743e-01         0.3275000                        0
## 22        22           1.561022e-01         0.3270588                        0
## 117      117           1.961213e-01         0.3270588                        0
## 364      364           1.399917e-01         0.3260274                        0
## 1093    1093           1.641347e-01         0.3260000                        0
## 1491    1491           1.840337e-01         0.3260000                        0
## 1551    1551           1.611788e-01         0.3260000                        0
## 1304    1304           1.915951e-01         0.3253731                        0
## 353      353           8.462807e-02         0.3251429                        0
## 1560    1560           9.495572e-02         0.3251429                        0
## 356      356           1.259533e-01         0.3248000                        0
## 924      924           1.661112e-01         0.3248000                        0
## 1643    1643           1.906470e-01         0.3248000                        0
## 363      363           1.852252e-01         0.3241379                        0
## 1557    1557           1.910047e-01         0.3240000                        0
## 512      512           1.508271e-01         0.3221818                        0
## 1118    1118           1.807922e-01         0.3202864                        0
## 861      861           1.488145e-01         0.3202186                        0
## 35        35           1.824827e-01         0.3200000                        0
## 615      615           1.380985e-01         0.3200000                        0
## 709      709           1.695266e-01         0.3200000                        0
## 948      948           1.796597e-01         0.3200000                        0
## 1035    1035           1.567640e-01         0.3200000                        0
## 1136    1136           1.274781e-01         0.3200000                        0
## 1312    1312           1.352754e-01         0.3200000                        0
## 1580    1580           1.996088e-01         0.3200000                        0
## 1762    1762           1.069740e-01         0.3200000                        0
## 933      933           1.698999e-01         0.3196250                        0
## 454      454           1.678323e-01         0.3187500                        0
## 1664    1664           1.609023e-01         0.3185185                        0
## 1349    1349           1.637443e-01         0.3183752                        0
## 971      971           1.398990e-01         0.3183673                        0
## 679      679           1.576603e-01         0.3176000                        0
## 265      265           1.733207e-01         0.3166667                        0
## 213      213           1.469367e-01         0.3100000                        0
## 1379    1379           1.775563e-01         0.3100000                        0
## 1155    1155           1.924393e-01         0.3090909                        0
## 610      610           1.706388e-01         0.3087500                        0
## 875      875           1.507099e-01         0.3087500                        0
## 1219    1219           1.448225e-01         0.3080000                        0
## 1406    1406           1.700079e-01         0.3080000                        0
## 1772    1772           1.584543e-01         0.3075200                        0
## 1617    1617           1.766227e-01         0.3069959                        0
## 1023    1023           1.590840e-01         0.3061538                        0
## 144      144           1.797751e-01         0.3051095                        0
## 719      719           1.341115e-01         0.3050000                        0
## 1280    1280           1.811507e-01         0.3050000                        0
## 851      851           1.205906e-01         0.3045714                        0
## 1123    1123           1.262817e-01         0.3028571                        0
## 976      976           1.637508e-01         0.3026316                        0
## 386      386           1.183132e-01         0.3025455                        0
## 958      958           1.739871e-01         0.3020000                        0
## 486      486           1.029958e-01         0.3000000                        0
## 988      988           1.718269e-01         0.3000000                        0
## 1409    1409           1.282932e-01         0.2990000                        0
## 872      872           1.328991e-01         0.2978947                        0
## 419      419           1.300943e-01         0.2970350                        0
## 270      270           1.491117e-01         0.2966667                        0
## 209      209           1.843520e-01         0.2960000                        0
## 420      420           1.390639e-01         0.2960000                        0
## 611      611           1.706439e-01         0.2960000                        0
## 1020    1020           1.514539e-01         0.2960000                        0
## 1446    1446           8.506601e-02         0.2960000                        0
## 1698    1698           1.606890e-01         0.2960000                        0
## 1793    1793           1.767766e-01         0.2960000                        0
## 923      923           1.556409e-01         0.2945000                        0
## 196      196           1.911500e-01         0.2937500                        0
## 112      112           1.166479e-01         0.2936000                        0
## 1224    1224           8.124078e-09         0.2936000                        0
## 119      119           1.961794e-01         0.2935733                        0
## 433      433           1.540704e-01         0.2934286                        0
## 734      734           1.894062e-01         0.2933333                        0
## 917      917           7.610772e-02         0.2931034                        0
## 1223    1223           1.685567e-01         0.2930000                        0
## 946      946           1.338980e-01         0.2920000                        0
## 1341    1341           1.734710e-01         0.2916800                        0
## 407      407           1.498381e-01         0.2912000                        0
## 964      964           1.943030e-01         0.2900000                        0
## 1384    1384           1.948920e-01         0.2900000                        0
## 1407    1407           1.860775e-01         0.2900000                        0
## 1421    1421           1.366316e-01         0.2900000                        0
## 1423    1423           1.826717e-01         0.2895522                        0
## 831      831           1.937795e-01         0.2888000                        0
## 1326    1326           1.301671e-01         0.2888000                        0
## 1200    1200           1.795820e-01         0.2884211                        0
## 629      629           9.776180e-02         0.2875294                        0
## 1205    1205           1.795796e-01         0.2864000                        0
## 987      987           1.414858e-01         0.2850000                        0
## 790      790           1.396635e-01         0.2840000                        0
## 1073    1073           1.250889e-01         0.2840000                        0
## 1364    1364           1.701113e-01         0.2840000                        0
## 1383    1383           1.842964e-01         0.2840000                        0
## 1254    1254           1.883437e-01         0.2834000                        0
## 128      128           1.400902e-01         0.2816000                        0
## 606      606           1.379463e-01         0.2816000                        0
## 1147    1147           1.728106e-01         0.2816000                        0
## 1708    1708           1.498569e-01         0.2816000                        0
## 163      163           1.930384e-01         0.2780000                        0
## 803      803           1.489007e-01         0.2780000                        0
## 846      846           1.823427e-01         0.2780000                        0
## 1729    1729           1.994167e-01         0.2780000                        0
## 907      907           1.347429e-01         0.2768000                        0
## 1277    1277           1.970875e-01         0.2763636                        0
## 567      567           1.696842e-01         0.2760000                        0
## 1415    1415           1.865441e-01         0.2750000                        0
## 1518    1518           1.370357e-01         0.2750000                        0
## 1571    1571           1.729785e-01         0.2750000                        0
## 1632    1632           1.838955e-01         0.2750000                        0
## 148      148           1.393828e-01         0.2744828                        0
## 1132    1132           1.855930e-01         0.2744000                        0
## 1465    1465           1.555283e-01         0.2744000                        0
## 1681    1681           1.424733e-01         0.2744000                        0
## 726      726           1.955758e-01         0.2735000                        0
## 798      798           1.578096e-01         0.2733714                        0
## 1229    1229           2.357446e-02         0.2727742                        0
## 34        34           1.691281e-01         0.2720000                        0
## 554      554           1.380232e-01         0.2720000                        0
## 1110    1110           1.846676e-01         0.2720000                        0
## 1112    1112           1.740532e-01         0.2720000                        0
## 1763    1763           1.182314e-01         0.2720000                        0
## 1076    1076           1.549100e-01         0.2710769                        0
## 678      678           1.182109e-01         0.2700000                        0
## 754      754           1.909750e-01         0.2700000                        0
## 1163    1163           1.824624e-01         0.2700000                        0
## 1733    1733           1.363685e-01         0.2700000                        0
## 692      692           1.830916e-01         0.2692308                        0
## 111      111           1.560071e-01         0.2690000                        0
## 700      700           1.135292e-01         0.2690000                        0
## 494      494           1.763766e-01         0.2672000                        0
## 569      569           1.311790e-01         0.2672000                        0
## 737      737           1.620767e-01         0.2672000                        0
## 1714    1714           8.976732e-02         0.2672000                        0
## 1434    1434           1.442480e-01         0.2667500                        0
## 738      738           1.719249e-01         0.2666667                        0
## 845      845           1.771788e-01         0.2665455                        0
## 1060    1060           1.630825e-01         0.2660000                        0
## 1731    1731           1.627323e-01         0.2660000                        0
## 281      281           1.590013e-01         0.2658605                        0
## 1790    1790           1.570907e-01         0.2655385                        0
## 357      357           1.975279e-01         0.2652243                        0
## 398      398           1.558566e-01         0.2651429                        0
## 336      336           1.910383e-01         0.2648000                        0
##            pnl    criterion
## 1334 0.6233333 5.050682e+00
## 715  0.6117647 4.340246e+00
## 182  0.5840000 3.085451e+00
## 864  0.5800000 3.009480e+00
## 1739 0.5650000 2.945007e+00
## 972  0.5491943 2.838746e+00
## 1722 0.5428571 2.798228e+00
## 1331 0.5360000 2.823350e+00
## 978  0.5240000 3.193335e+00
## 208  0.5200000 2.709778e+00
## 1647 0.5094737 2.847953e+00
## 247  0.5048000 2.705316e+00
## 1016 0.5048000 2.600693e+00
## 65   0.5000000 1.593836e+01
## 146  0.4880000 2.675303e+00
## 259  0.4863636 2.664586e+00
## 1612 0.4820000 2.682744e+00
## 480  0.4447368 2.596400e+00
## 1765 0.4357143 2.222722e+00
## 465  0.4352000 2.489240e+00
## 378  0.4350000 2.175931e+00
## 1738 0.4333333 2.488535e+00
## 739  0.4200000 2.288618e+00
## 20   0.4160000 2.131789e+00
## 263  0.3944000 2.891955e+00
## 418  0.3896000 2.958405e+00
## 266  0.3848000 3.533038e+00
## 729  0.3800000 2.143090e+00
## 1148 0.3800000 9.595966e+00
## 1567 0.3800000 2.378041e+00
## 1514 0.3776000 1.911260e+00
## 467  0.3728000 2.237790e+00
## 1220 0.3728000 2.327664e+00
## 168  0.3656000 2.033174e+00
## 867  0.3645714 2.160779e+00
## 1500 0.3640000 2.364412e+00
## 10   0.3565217 2.054303e+00
## 341  0.3560000 4.038780e+00
## 1608 0.3560000 1.943467e+00
## 123  0.3550000 2.059732e+00
## 7    0.3545455 1.805114e+00
## 416  0.3538462 1.886430e+00
## 767  0.3521951 1.835809e+00
## 354  0.3480000 2.272253e+00
## 929  0.3474286 3.844536e+00
## 1295 0.3470000 2.345446e+00
## 118  0.3450000 2.162251e+00
## 1105 0.3432836 1.911432e+00
## 714  0.3360825 1.979816e+00
## 1350 0.3350000 2.600877e+00
## 717  0.3336634 2.119510e+00
## 179  0.3328571 2.179719e+00
## 30   0.3320000 2.712647e+00
## 256  0.3300000 1.994993e+00
## 1024 0.3296000 3.103142e+00
## 1646 0.3296000 2.458789e+00
## 1294 0.3284000 1.880819e+00
## 1761 0.3283721 1.733173e+00
## 832  0.3275000 2.216217e+00
## 22   0.3270588 2.095158e+00
## 117  0.3270588 1.667636e+00
## 364  0.3260274 2.328905e+00
## 1093 0.3260000 1.986173e+00
## 1491 0.3260000 1.771415e+00
## 1551 0.3260000 2.022598e+00
## 1304 0.3253731 1.698233e+00
## 353  0.3251429 3.842021e+00
## 1560 0.3251429 3.424152e+00
## 356  0.3248000 2.578733e+00
## 924  0.3248000 1.955317e+00
## 1643 0.3248000 1.703672e+00
## 363  0.3241379 1.749966e+00
## 1557 0.3240000 1.696293e+00
## 512  0.3221818 2.136100e+00
## 1118 0.3202864 1.771572e+00
## 861  0.3202186 2.151796e+00
## 35   0.3200000 1.753591e+00
## 615  0.3200000 2.317187e+00
## 709  0.3200000 1.887609e+00
## 948  0.3200000 1.781145e+00
## 1035 0.3200000 2.041285e+00
## 1136 0.3200000 2.510235e+00
## 1312 0.3200000 2.365545e+00
## 1580 0.3200000 1.603135e+00
## 1762 0.3200000 2.991381e+00
## 933  0.3196250 1.881255e+00
## 454  0.3187500 1.899217e+00
## 1664 0.3185185 1.979578e+00
## 1349 0.3183752 1.944344e+00
## 971  0.3183673 2.275694e+00
## 679  0.3176000 2.014458e+00
## 265  0.3166667 1.827056e+00
## 213  0.3100000 2.109752e+00
## 1379 0.3100000 1.745925e+00
## 1155 0.3090909 1.606173e+00
## 610  0.3087500 1.809377e+00
## 875  0.3087500 2.048638e+00
## 1219 0.3080000 2.126741e+00
## 1406 0.3080000 1.811681e+00
## 1772 0.3075200 1.940748e+00
## 1617 0.3069959 1.738145e+00
## 1023 0.3061538 1.924479e+00
## 144  0.3051095 1.697173e+00
## 719  0.3050000 2.274228e+00
## 1280 0.3050000 1.683681e+00
## 851  0.3045714 2.525665e+00
## 1123 0.3028571 2.398266e+00
## 976  0.3026316 1.848122e+00
## 386  0.3025455 2.557156e+00
## 958  0.3020000 1.735761e+00
## 486  0.3000000 2.912739e+00
## 988  0.3000000 1.745943e+00
## 1409 0.2990000 2.330600e+00
## 872  0.2978947 2.241510e+00
## 419  0.2970350 2.283230e+00
## 270  0.2966667 1.989559e+00
## 209  0.2960000 1.605624e+00
## 420  0.2960000 2.128518e+00
## 611  0.2960000 1.734606e+00
## 1020 0.2960000 1.954390e+00
## 1446 0.2960000 3.479651e+00
## 1698 0.2960000 1.842068e+00
## 1793 0.2960000 1.674430e+00
## 923  0.2945000 1.892176e+00
## 196  0.2937500 1.536751e+00
## 112  0.2936000 2.516977e+00
## 1224 0.2936000 3.613949e+07
## 119  0.2935733 1.496453e+00
## 433  0.2934286 1.904509e+00
## 734  0.2933333 1.548700e+00
## 917  0.2931034 3.851166e+00
## 1223 0.2930000 1.738287e+00
## 946  0.2920000 2.180765e+00
## 1341 0.2916800 1.681434e+00
## 407  0.2912000 1.943431e+00
## 964  0.2900000 1.492515e+00
## 1384 0.2900000 1.488003e+00
## 1407 0.2900000 1.558490e+00
## 1421 0.2900000 2.122496e+00
## 1423 0.2895522 1.585096e+00
## 831  0.2888000 1.490354e+00
## 1326 0.2888000 2.218686e+00
## 1200 0.2884211 1.606069e+00
## 629  0.2875294 2.941122e+00
## 1205 0.2864000 1.594836e+00
## 987  0.2850000 2.014336e+00
## 790  0.2840000 2.033459e+00
## 1073 0.2840000 2.270386e+00
## 1364 0.2840000 1.669496e+00
## 1383 0.2840000 1.540996e+00
## 1254 0.2834000 1.504696e+00
## 128  0.2816000 2.010133e+00
## 606  0.2816000 2.041374e+00
## 1147 0.2816000 1.629529e+00
## 1708 0.2816000 1.879126e+00
## 163  0.2780000 1.440128e+00
## 803  0.2780000 1.867016e+00
## 846  0.2780000 1.524602e+00
## 1729 0.2780000 1.394066e+00
## 907  0.2768000 2.054283e+00
## 1277 0.2763636 1.402238e+00
## 567  0.2760000 1.626551e+00
## 1415 0.2750000 1.474183e+00
## 1518 0.2750000 2.006776e+00
## 1571 0.2750000 1.589793e+00
## 1632 0.2750000 1.495415e+00
## 148  0.2744828 1.969273e+00
## 1132 0.2744000 1.478504e+00
## 1465 0.2744000 1.764309e+00
## 1681 0.2744000 1.925974e+00
## 726  0.2735000 1.398435e+00
## 798  0.2733714 1.732287e+00
## 1229 0.2727742 1.157075e+01
## 34   0.2720000 1.608248e+00
## 554  0.2720000 1.970683e+00
## 1110 0.2720000 1.472917e+00
## 1112 0.2720000 1.562740e+00
## 1763 0.2720000 2.300574e+00
## 1076 0.2710769 1.749899e+00
## 678  0.2700000 2.284053e+00
## 754  0.2700000 1.413797e+00
## 1163 0.2700000 1.479757e+00
## 1733 0.2700000 1.979929e+00
## 692  0.2692308 1.470470e+00
## 111  0.2690000 1.724281e+00
## 700  0.2690000 2.369434e+00
## 494  0.2672000 1.514940e+00
## 569  0.2672000 2.036912e+00
## 737  0.2672000 1.648602e+00
## 1714 0.2672000 2.976584e+00
## 1434 0.2667500 1.849245e+00
## 738  0.2666667 1.551065e+00
## 845  0.2665455 1.504387e+00
## 1060 0.2660000 1.631077e+00
## 1731 0.2660000 1.634587e+00
## 281  0.2658605 1.672065e+00
## 1790 0.2655385 1.690351e+00
## 357  0.2652243 1.342718e+00
## 398  0.2651429 1.701197e+00
## 336  0.2648000 1.386110e+00</code></pre>
<pre class="r"><code>top_200_pnl_loan_nb &lt;- top_200_pnl$loan_nb
top_200_pnl_loan_nb_df &lt;- data.frame(top_200_pnl_loan_nb)
print(top_200_pnl_loan_nb_df)</code></pre>
<pre><code>##     top_200_pnl_loan_nb
## 1                  1334
## 2                   715
## 3                   182
## 4                   864
## 5                  1739
## 6                   972
## 7                  1722
## 8                  1331
## 9                   978
## 10                  208
## 11                 1647
## 12                  247
## 13                 1016
## 14                   65
## 15                  146
## 16                  259
## 17                 1612
## 18                  480
## 19                 1765
## 20                  465
## 21                  378
## 22                 1738
## 23                  739
## 24                   20
## 25                  263
## 26                  418
## 27                  266
## 28                  729
## 29                 1148
## 30                 1567
## 31                 1514
## 32                  467
## 33                 1220
## 34                  168
## 35                  867
## 36                 1500
## 37                   10
## 38                  341
## 39                 1608
## 40                  123
## 41                    7
## 42                  416
## 43                  767
## 44                  354
## 45                  929
## 46                 1295
## 47                  118
## 48                 1105
## 49                  714
## 50                 1350
## 51                  717
## 52                  179
## 53                   30
## 54                  256
## 55                 1024
## 56                 1646
## 57                 1294
## 58                 1761
## 59                  832
## 60                   22
## 61                  117
## 62                  364
## 63                 1093
## 64                 1491
## 65                 1551
## 66                 1304
## 67                  353
## 68                 1560
## 69                  356
## 70                  924
## 71                 1643
## 72                  363
## 73                 1557
## 74                  512
## 75                 1118
## 76                  861
## 77                   35
## 78                  615
## 79                  709
## 80                  948
## 81                 1035
## 82                 1136
## 83                 1312
## 84                 1580
## 85                 1762
## 86                  933
## 87                  454
## 88                 1664
## 89                 1349
## 90                  971
## 91                  679
## 92                  265
## 93                  213
## 94                 1379
## 95                 1155
## 96                  610
## 97                  875
## 98                 1219
## 99                 1406
## 100                1772
## 101                1617
## 102                1023
## 103                 144
## 104                 719
## 105                1280
## 106                 851
## 107                1123
## 108                 976
## 109                 386
## 110                 958
## 111                 486
## 112                 988
## 113                1409
## 114                 872
## 115                 419
## 116                 270
## 117                 209
## 118                 420
## 119                 611
## 120                1020
## 121                1446
## 122                1698
## 123                1793
## 124                 923
## 125                 196
## 126                 112
## 127                1224
## 128                 119
## 129                 433
## 130                 734
## 131                 917
## 132                1223
## 133                 946
## 134                1341
## 135                 407
## 136                 964
## 137                1384
## 138                1407
## 139                1421
## 140                1423
## 141                 831
## 142                1326
## 143                1200
## 144                 629
## 145                1205
## 146                 987
## 147                 790
## 148                1073
## 149                1364
## 150                1383
## 151                1254
## 152                 128
## 153                 606
## 154                1147
## 155                1708
## 156                 163
## 157                 803
## 158                 846
## 159                1729
## 160                 907
## 161                1277
## 162                 567
## 163                1415
## 164                1518
## 165                1571
## 166                1632
## 167                 148
## 168                1132
## 169                1465
## 170                1681
## 171                 726
## 172                 798
## 173                1229
## 174                  34
## 175                 554
## 176                1110
## 177                1112
## 178                1763
## 179                1076
## 180                 678
## 181                 754
## 182                1163
## 183                1733
## 184                 692
## 185                 111
## 186                 700
## 187                 494
## 188                 569
## 189                 737
## 190                1714
## 191                1434
## 192                 738
## 193                 845
## 194                1060
## 195                1731
## 196                 281
## 197                1790
## 198                 357
## 199                 398
## 200                 336</code></pre>
<pre class="r"><code>#arrange the table using our criterion that weight the predicted probability of default and the return if no default
pred_logistic_reg_assessment_data_df_complete_ordered_by_criterion &lt;- arrange(pred_logistic_reg_assessment_data_df_complete,desc(criterion))
  
#print the first 200 loans which would be the ones we would invest in according to our criterion
top_200_criterion &lt;- head(pred_logistic_reg_assessment_data_df_complete_ordered_by_criterion,200)
  by_criterion_total_pnl_no_default &lt;- sum(top_200_criterion$return_no_default)
  by_criterion_total_pnl_no_default</code></pre>
<pre><code>## [1] 41.23098</code></pre>
<pre class="r"><code>top_200_criterion_loan_nb &lt;- top_200_criterion$loan_nb
top_200_criterion_loan_nb_df &lt;- data.frame(top_200_criterion_loan_nb)
print(top_200_criterion_loan_nb_df)</code></pre>
<pre><code>##     top_200_criterion_loan_nb
## 1                        1224
## 2                          65
## 3                        1229
## 4                        1148
## 5                        1318
## 6                         704
## 7                        1319
## 8                         990
## 9                        1593
## 10                        318
## 11                       1334
## 12                       1302
## 13                          9
## 14                        715
## 15                        702
## 16                       1648
## 17                        440
## 18                        974
## 19                        779
## 20                        935
## 21                        515
## 22                        341
## 23                       1103
## 24                        917
## 25                        929
## 26                        353
## 27                         21
## 28                       1393
## 29                        360
## 30                       1288
## 31                        932
## 32                        307
## 33                         64
## 34                        243
## 35                        722
## 36                        473
## 37                        266
## 38                       1018
## 39                        913
## 40                        620
## 41                        469
## 42                       1446
## 43                       1293
## 44                       1247
## 45                       1242
## 46                        947
## 47                       1560
## 48                        680
## 49                        637
## 50                        544
## 51                       1463
## 52                        941
## 53                        520
## 54                       1044
## 55                       1786
## 56                       1398
## 57                       1443
## 58                        978
## 59                       1342
## 60                         83
## 61                       1007
## 62                       1445
## 63                        504
## 64                       1043
## 65                        424
## 66                       1189
## 67                        545
## 68                       1307
## 69                         75
## 70                       1654
## 71                       1024
## 72                       1576
## 73                        182
## 74                         53
## 75                       1226
## 76                       1710
## 77                        842
## 78                        232
## 79                       1262
## 80                       1248
## 81                       1255
## 82                       1051
## 83                       1638
## 84                        190
## 85                        864
## 86                        693
## 87                         45
## 88                       1762
## 89                        425
## 90                        552
## 91                       1714
## 92                        340
## 93                        821
## 94                        418
## 95                       1078
## 96                        687
## 97                        457
## 98                       1739
## 99                        629
## 100                       320
## 101                       804
## 102                       486
## 103                      1707
## 104                       290
## 105                      1564
## 106                      1128
## 107                       263
## 108                      1137
## 109                      1077
## 110                       217
## 111                        80
## 112                      1488
## 113                       527
## 114                       764
## 115                      1647
## 116                      1198
## 117                       151
## 118                       345
## 119                       972
## 120                        61
## 121                      1343
## 122                      1331
## 123                      1054
## 124                      1258
## 125                        69
## 126                       765
## 127                      1340
## 128                       408
## 129                      1722
## 130                      1359
## 131                       250
## 132                       628
## 133                      1563
## 134                      1328
## 135                       814
## 136                        46
## 137                      1448
## 138                       871
## 139                       596
## 140                      1752
## 141                      1067
## 142                        50
## 143                      1586
## 144                       559
## 145                        79
## 146                       221
## 147                        30
## 148                       208
## 149                       649
## 150                      1061
## 151                       247
## 152                       241
## 153                       129
## 154                      1504
## 155                      1794
## 156                       556
## 157                      1770
## 158                      1791
## 159                      1612
## 160                       146
## 161                       901
## 162                       592
## 163                       462
## 164                       261
## 165                       259
## 166                       533
## 167                        99
## 168                       200
## 169                       940
## 170                      1057
## 171                      1152
## 172                       157
## 173                       730
## 174                       450
## 175                      1350
## 176                      1016
## 177                      1732
## 178                       480
## 179                       448
## 180                        77
## 181                      1456
## 182                      1310
## 183                        14
## 184                       356
## 185                      1480
## 186                      1072
## 187                       820
## 188                       451
## 189                       386
## 190                       575
## 191                       926
## 192                       640
## 193                      1641
## 194                      1630
## 195                      1021
## 196                       638
## 197                       851
## 198                      1529
## 199                       246
## 200                       616</code></pre>
<p>Note that the <code>echo = FALSE</code> parameter was added to the code chunk to prevent printing of the R code that generated the plot.</p>
</div>
</div>
